{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_l2beat import AdapterL2Beat\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['optimism','arbitrum'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterL2Beat(adapter_params, db_connector)\n",
    "# extract\n",
    "df= ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefiLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_defillama import AdapterDefiLlama\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['ethereum'] # see all options in adapter_mapping.py\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDefiLlama(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coingecko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for projects / chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'project',\n",
    "    'metric_keys' : ['price', 'volume', 'market_cap'],\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['aptos'], # see all options in adapter_mapping.py\n",
    "    'days' : 'auto', # auto, max, or a number (as string)\n",
    "    'vs_currencies' : ['usd', 'eth']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for imx tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'imx_tokens'\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    # 'query_names' : ['waa'], ## fundamentals, waa, stables_mcap\n",
    "    # 'days' : 100,\n",
    "    'query_names' : None,\n",
    "    'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside\n",
    "sometimes some Flipside queries just get stuck -- gotta retrigger them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_flipside import AdapterFlipside\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : ['zksync_era'],\n",
    "    'metric_keys' : ['stables_mcap'],\n",
    "    'days' : 'auto',\n",
    "    # 'origin_keys' : None,\n",
    "    # 'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipside(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the new flipside api\n",
    "\n",
    "from flipside import Flipside\n",
    "flipside = Flipside(\"74c43ebc-3291-4953-8aeb-65640da7c852\", \"https://api-v2.flipsidecrypto.xyz\")\n",
    "\n",
    "sql = \"\"\"\n",
    " select \n",
    "    BLOCK_NUMBER, BLOCK_TIMESTAMP, BLOCK_HASH, TX_HASH, NONCE, POSITION, ORIGIN_FUNCTION_SIGNATURE, FROM_ADDRESS, TO_ADDRESS, ETH_VALUE, TX_FEE, GAS_PRICE, GAS_LIMIT, \n",
    "    GAS_USED, L1_GAS_PRICE, L1_GAS_USED, L1_FEE_SCALAR, L1_SUBMISSION_BATCH_INDEX, L1_SUBMISSION_TX_HASH, L1_STATE_ROOT_BATCH_INDEX, \n",
    "    L1_STATE_ROOT_TX_HASH, CUMULATIVE_GAS_USED, INPUT_DATA, STATUS\n",
    "from optimism.core.fact_transactions\n",
    "where block_number >= 104130000\n",
    "and block_number < 104135000\n",
    "order by block_number asc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query against Flipside's query engine and await the results\n",
    "query_result_set = flipside.query(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zettablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter ZettaBlock initialized with {'api_key': '***'}.\n",
      "...triggered polygon_zkevm - txcount with days: 10. run_id: ar21-89ddb62a-89a2-430e-af15-a6c1a06d1cea\n",
      "...triggered polygon_zkevm - daa with days: 10. run_id: ar21-fdff8477-668c-4af2-a3a5-f95e0e65dc02\n",
      "...triggered polygon_zkevm - fees_paid_usd with days: 10. run_id: ar21-aab981ef-bb84-4840-8627-6a400f9ac3d0\n",
      "...triggered polygon_zkevm - txcosts_median_usd with days: 10. run_id: ar21-cf9b233b-f794-4286-bd6d-f0ef1210804b\n",
      "...triggered zksync_era - txcount with days: 10. run_id: ar21-a3ea4903-8764-49f2-b64a-d48289a7e3c8\n",
      "...triggered zksync_era - daa with days: 10. run_id: ar21-2c4b174a-5e1d-4ab4-b392-a5d4f0a5eb31\n",
      "...triggered zksync_era - fees_paid_usd with days: 10. run_id: ar21-d2d413a2-5fc3-4ad2-8840-78d73ed3d1ef\n",
      "...triggered zksync_era - txcosts_median_usd with days: 10. run_id: ar21-38342917-635c-4c40-b84a-5ba98e3bda94\n",
      "...done polygon_zkevm - txcount\n",
      "...done polygon_zkevm - daa\n",
      "...done polygon_zkevm - fees_paid_usd\n",
      "...done polygon_zkevm - txcosts_median_usd\n",
      "...done zksync_era - txcount\n",
      "...done zksync_era - daa\n",
      "...done zksync_era - fees_paid_usd\n",
      "...done zksync_era - txcosts_median_usd\n",
      "... ALL queries finished execution.\n",
      "ZettaBlock extract done for {'days': 10, 'origin_keys': None, 'metric_keys': None}. DataFrame shape: (80, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_zettablock import AdapterZettablock\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : ['zksync_era', 'polygon_zkevm'],\n",
    "    #'metric_keys' : ['txcount', ''],\n",
    "    'days' : 10,\n",
    "    'origin_keys' : None,\n",
    "    'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettablock(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZettaBlock raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter ZettablockRaw initialized with {'api_key': '***'}.\n",
      "... finding latest block in ZettaBlock for zksync_era_tx with query_run_id: ar22-350ebf53-236f-4e2e-9ef9-0e853e1e567a\n",
      "Current max block for zksync_era_tx in ZettaBlock database is 9230488\n",
      "...start loading raw data for zksync_era_tx with block_start: 9069700\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-8104e9cd-c0d2-4b5c-8200-afb3e45e1435. With block_start: 9069700\n",
      "...loaded 18454 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9070699\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-80d97523-406e-4d7c-b81d-511c8ca52a72. With block_start: 9070699\n",
      "...loaded 19876 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9071698\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-58f228c2-cab1-4fc3-b136-1c316200e927. With block_start: 9071698\n",
      "...loaded 19918 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9069700-9072697\n",
      "...upserted 58216 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9072697\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b9e6fff2-a8f7-4b62-b2ed-3e1ef24ce9a1. With block_start: 9072697\n",
      "...loaded 19405 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9073696\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-fd17ef2a-c5ef-4e7a-a205-88d74375b18f. With block_start: 9073696\n",
      "...loaded 19101 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9074695\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-3b6d1220-0e8c-4331-8d42-f93e1777112d. With block_start: 9074695\n",
      "...loaded 18555 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9072697-9075694\n",
      "...upserted 57031 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9075694\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-cdc629ae-8e79-4071-930a-37ac9a655a8e. With block_start: 9075694\n",
      "...loaded 18562 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9076693\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-50cb7eb9-fce6-4f34-ba3d-648e34df3153. With block_start: 9076693\n",
      "...loaded 16841 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9077692\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-68eedaf0-e2e4-421f-a46a-57d482ec09c3. With block_start: 9077692\n",
      "...loaded 13699 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9078691\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-df0b06cd-5114-487e-9b08-05c37744e590. With block_start: 9078691\n",
      "...loaded 13632 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9075694-9079690\n",
      "...upserted 62693 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9079690\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-1a3b186c-d7cc-48cd-91be-d2e6b986c823. With block_start: 9079690\n",
      "...loaded 15323 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9080689\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-97c2948c-c4f9-43dd-a3ff-2bd086320ffa. With block_start: 9080689\n",
      "...loaded 16258 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9081688\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-7ab6a676-0b8a-4538-945a-d2ed41e62816. With block_start: 9081688\n",
      "...loaded 16806 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9082686\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-8742da34-d26d-4e31-bc04-2ba5f109dc87. With block_start: 9082686\n",
      "...loaded 14412 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9079690-9083685\n",
      "...upserted 62763 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9083685\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-cd725d5f-a7a2-43f4-9e88-2b9643315498. With block_start: 9083685\n",
      "...loaded 12370 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9084684\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b13406f3-7bd1-4d21-a00b-5bb7b6c18345. With block_start: 9084684\n",
      "...loaded 11013 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9085683\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-9a247836-dad1-4590-af0f-1f58b0f83f35. With block_start: 9085683\n",
      "...loaded 10736 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9086682\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-067d2909-e091-4775-8a2b-19b63646de7c. With block_start: 9086682\n",
      "...loaded 9855 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9087681\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-3e66d98e-37e2-409b-a7b6-81d382993094. With block_start: 9087681\n",
      "...loaded 8897 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9083685-9088680\n",
      "...upserted 52828 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9088680\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-286c921c-5ea7-4248-af5a-c2b3dde41553. With block_start: 9088680\n",
      "...loaded 7962 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9089679\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b2f801ff-d9ae-44f9-b5cf-0c001176d9b9. With block_start: 9089679\n",
      "...loaded 8236 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9090678\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-a8b10ffa-79e9-4c71-9855-e2edf04987f8. With block_start: 9090678\n",
      "...loaded 9397 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9091677\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-81969e2d-70bc-40fa-b001-5532c68a0f9d. With block_start: 9091677\n",
      "...loaded 7084 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9092676\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b52d04b3-ee63-4102-9711-ed1022322df1. With block_start: 9092676\n",
      "...loaded 6550 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9093675\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-4df9b224-6d21-4089-b392-29afae19a54b. With block_start: 9093675\n",
      "...loaded 6825 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9094674\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-66044ce9-f7dd-473f-9631-9c7c5bcb2d04. With block_start: 9094674\n",
      "...loaded 5258 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9088680-9095673\n",
      "...upserted 51276 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9095673\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-10ed9e6d-28a9-47b7-905a-b1eb255d0ff3. With block_start: 9095673\n",
      "...loaded 4180 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9096672\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-f0904d89-9aad-4654-aa8b-1c5a1dbc69ff. With block_start: 9096672\n",
      "...loaded 4183 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9097671\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-1509adc7-1aea-42f7-ae6e-07c288b0ec63. With block_start: 9097671\n",
      "...loaded 4038 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9098670\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-65184609-f45a-4585-a53b-977d7bddde6b. With block_start: 9098670\n",
      "...loaded 3768 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9099669\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-ed3723a4-511b-481a-88c6-4baf0e56200d. With block_start: 9099669\n",
      "...loaded 4233 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9100668\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-f7c22bd3-a4e5-4099-af22-2b35532f712c. With block_start: 9100668\n",
      "...loaded 3507 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9101667\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-9729e54a-50ef-46f0-a813-871d41d56a4b. With block_start: 9101667\n",
      "...loaded 3270 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9102666\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-17659ff4-c22d-4047-ac28-f86b2601a34a. With block_start: 9102666\n",
      "...loaded 3003 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9103665\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-3c211317-c71b-44b7-b9a5-d5c67bb3bf54. With block_start: 9103665\n",
      "...loaded 3021 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9104664\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-25092e42-9111-49cf-94b3-bf64fddbe4e9. With block_start: 9104664\n",
      "...loaded 3300 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9105663\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-de5ddfba-ec68-4887-8cf8-ae5820db667c. With block_start: 9105663\n",
      "...loaded 3339 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9106662\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-0a50c04e-47a6-4cd5-831b-8063cd3bbd21. With block_start: 9106662\n",
      "...loaded 3733 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9107661\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-eb07e027-d1a5-4288-a3f8-506fed3e85aa. With block_start: 9107661\n",
      "...loaded 3267 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9108660\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-85361bf7-fb04-42e4-8cc8-db168beee07f. With block_start: 9108660\n",
      "...loaded 3280 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9095673-9109659\n",
      "...upserted 50085 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9109659\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-c04cda06-72f8-445c-a805-4b8cb64a8a66. With block_start: 9109659\n",
      "...loaded 3335 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9110658\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-724e23e7-1c47-455d-b0e4-7496e670c12c. With block_start: 9110658\n",
      "...loaded 3095 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9111657\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-304e7468-f28c-4a38-9ff7-bddd96b82e8a. With block_start: 9111657\n",
      "...loaded 3145 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9112656\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-08a40a11-44dd-49ee-9e4a-d7f09dd0d902. With block_start: 9112656\n",
      "...loaded 3195 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9113655\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-60954049-1812-4535-9a7c-ac1a77bcc040. With block_start: 9113655\n",
      "...loaded 3358 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9114654\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b2ae819b-c146-4565-ac9f-d2cb1c5662d8. With block_start: 9114654\n",
      "...loaded 2794 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9115653\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-a13e7e77-44df-4f9f-a0e1-352bc4f39372. With block_start: 9115653\n",
      "...loaded 2595 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9116652\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-935d9edd-f5b3-4027-bce6-0021df1c4964. With block_start: 9116652\n",
      "...loaded 3232 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9117651\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-fe71a99d-cbf9-4c99-8a3d-19b5b664318c. With block_start: 9117651\n",
      "...loaded 3115 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9118650\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-5a54b515-8f2b-4745-99c3-f37df3f071f6. With block_start: 9118650\n",
      "...loaded 3509 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9119649\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-1f293e88-d909-4e04-bcad-75631fc7eb3c. With block_start: 9119649\n",
      "...loaded 3580 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9120648\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-a6f24b83-c3ae-4130-821f-33f382dd8f54. With block_start: 9120648\n",
      "...loaded 3129 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9121647\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-62be60e1-fef5-47ce-90f0-cf352e4bc0e3. With block_start: 9121647\n",
      "...loaded 3389 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9122646\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-4a860070-298d-4477-92fc-59b32dfb2653. With block_start: 9122646\n",
      "...loaded 4102 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9123645\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-8ca52335-3c2c-4077-bc7a-8d93e2089761. With block_start: 9123645\n",
      "...loaded 3637 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9124644\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-8bccdcf0-057c-4bb8-b177-734f9ca3845c. With block_start: 9124644\n",
      "...loaded 4173 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9109659-9125643\n",
      "...upserted 53338 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9125643\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-2412b4f7-e313-4fb4-be53-5bc46630ecdf. With block_start: 9125643\n",
      "...loaded 4732 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9126642\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-7a5e2eaa-354d-4fdd-aaa7-d62363a6d536. With block_start: 9126642\n",
      "...loaded 5441 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9127641\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-ef4e8436-19f0-452f-8922-f6ee30eedcaf. With block_start: 9127641\n",
      "...loaded 5819 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9128640\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-2c06a59c-b950-4ba8-ac59-ac2b719555ef. With block_start: 9128640\n",
      "...loaded 6552 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9129639\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-972f1f83-f7ca-4b60-a1a3-90bf599d112b. With block_start: 9129639\n",
      "...loaded 7754 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9130638\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b9ed24fd-1cf6-4819-ac74-bfd91eb00e75. With block_start: 9130638\n",
      "...loaded 8350 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9131637\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-e1171914-456e-4444-961e-d8801da027a2. With block_start: 9131637\n",
      "...loaded 8878 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9132636\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-af34227f-15aa-4109-bd08-977cfeedf32b. With block_start: 9132636\n",
      "...loaded 10021 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9125643-9133635\n",
      "...upserted 57499 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9133635\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-402dfb20-33c7-428c-b446-dc592263f3aa. With block_start: 9133635\n",
      "...loaded 11394 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9134634\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-602f228f-650e-4ad1-a27d-34a8262aa590. With block_start: 9134634\n",
      "...loaded 12267 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9135633\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-b7bacd56-b4cd-48c1-9ad8-39fcdcf9d878. With block_start: 9135633\n",
      "...loaded 13576 rows for zksync_era_tx\n",
      "...start loading raw data for zksync_era_tx with block_start: 9136632\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-a40f15ac-d4d5-4a5d-b14a-30f217951ee5. With block_start: 9136632\n",
      "...loaded 14273 rows for zksync_era_tx\n",
      "...loaded more than 50k rows for zksync_era_tx, trigger upload\n",
      "...uploaded to S3 longterm in zksync_era/zksync_era_tx_9133635-9137631\n",
      "...upserted 51474 rows to zksync_era_tx table\n",
      "...start loading raw data for zksync_era_tx with block_start: 9137631\n",
      "... triggerd query_id: qu17-0d6b9c22-5df6-4f42-9812-3fd6fbfb40db with query_run_id: ar22-2f6ee217-06a1-4721-bf9d-38b7f69a24ca. With block_start: 9137631\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\MyLocal\\gtp\\backend\\nb_data_load.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Desktop/MyLocal/gtp/backend/nb_data_load.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ad \u001b[39m=\u001b[39m AdapterZettaBlockRaw(adapter_params, db_connector)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Desktop/MyLocal/gtp/backend/nb_data_load.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# extract\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Desktop/MyLocal/gtp/backend/nb_data_load.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ad\u001b[39m.\u001b[39;49mextract_raw(load_params, if_exists \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mignore\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\MyLocal\\gtp\\backend\\src\\adapters\\adapter_raw_zettablock.py:36\u001b[0m, in \u001b[0;36mAdapterZettaBlockRaw.extract_raw\u001b[1;34m(self, load_params, if_exists)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries_to_load \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m zettablock_raws \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mkey \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys]\n\u001b[0;32m     35\u001b[0m \u001b[39m## Trigger queries\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrigger_check_extract_queries(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqueries_to_load, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_start, if_exists)\n\u001b[0;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\MyLocal\\gtp\\backend\\src\\adapters\\adapter_raw_zettablock.py:71\u001b[0m, in \u001b[0;36mAdapterZettaBlockRaw.trigger_check_extract_queries\u001b[1;34m(self, queries_to_load, block_start, if_exists)\u001b[0m\n\u001b[0;32m     68\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m3\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[39m## wait till query done                  \u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_till_query_done(query\u001b[39m.\u001b[39;49mlast_run_id)\n\u001b[0;32m     73\u001b[0m \u001b[39m## get query results\u001b[39;00m\n\u001b[0;32m     74\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mget_query_results(query\u001b[39m.\u001b[39mlast_run_id)\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\MyLocal\\gtp\\backend\\src\\adapters\\adapter_raw_zettablock.py:108\u001b[0m, in \u001b[0;36mAdapterZettaBlockRaw.wait_till_query_done\u001b[1;34m(self, queryrun_id)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait_till_query_done\u001b[39m(\u001b[39mself\u001b[39m, queryrun_id):\n\u001b[0;32m    107\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:   \n\u001b[1;32m--> 108\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcheck_query_execution(queryrun_id)\n\u001b[0;32m    109\u001b[0m         \u001b[39mif\u001b[39;00m res \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive\\Desktop\\MyLocal\\gtp\\backend\\src\\adapters\\clients\\zettablock_api.py:29\u001b[0m, in \u001b[0;36mZettaBlock_API.check_query_execution\u001b[1;34m(self, queryrun_id)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_query_execution\u001b[39m(\u001b[39mself\u001b[39m, queryrun_id):\n\u001b[0;32m     27\u001b[0m     queryrun_status_endpoint \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url\u001b[39m}\u001b[39;00m\u001b[39m/queryruns/\u001b[39m\u001b[39m{\u001b[39;00mqueryrun_id\u001b[39m}\u001b[39;00m\u001b[39m/status\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 29\u001b[0m     res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(queryrun_status_endpoint, headers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheader)\n\u001b[0;32m     30\u001b[0m     response_json \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(res\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m response_json[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSUCCEEDED\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    484\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    486\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 487\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    488\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    489\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    490\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    491\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    492\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    497\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    498\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_zettablock import AdapterZettaBlockRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    #'keys' : ['polygon_zkevm_tx', 'zksync_era_tx'],\n",
    "    'keys' : ['zksync_era_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettaBlockRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params, if_exists = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainbase raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_chainbase import AdapterChainbaseRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"CHAINBASE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 96528664,\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterChainbaseRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMX raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement orchestration?\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_imx import AdapterRawImx\n",
    "\n",
    "adapter_params = {\n",
    "    'load_types' : ['withdrawals', 'deposits', 'trades', 'orders_filled', 'transfers', 'mints'],\n",
    "    'forced_refresh' : 'no',\n",
    "\n",
    "    #'load_types' : ['orders_filled'],\n",
    "    #'forced_refresh' : '2023-04-01T00:00:00.000000Z',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRawImx(adapter_params, db_connector)\n",
    "# extract raw (and load raw in case of IMX)\n",
    "df_raw = ad.extract_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_flipside import AdapterFlipsideRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx', 'optimism_tx'],\n",
    "    'block_start' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipsideRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads currently IMX txcount, daa, fees_paid\n",
    "## also loads user_base_weekly\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics', ## usd_to_eth or metrics\n",
    "    \n",
    "    #'days' : 'auto', ## days as int our 'auto\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    #'metric_keys' : None, ## metric_keys as list or None\n",
    "\n",
    "    'days' : '28', ## days as int our 'auto\n",
    "    #'origin_keys' : ['imx'], ## origin_keys as list or None\n",
    "    'metric_keys' : ['user_base_weekly'], ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.origin_key.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'usd_to_eth', ## usd_to_eth or metrics\n",
    "    'days' : 180, ## days as int\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockspace logic\n",
    "- for each chain, aggregate the daily contracts usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "\n",
    "chain_list = ['polygon_zkevm']\n",
    "days = 300\n",
    "chain = 'polygon_zkevm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregagting contract data for polygon_zkevm and last 300 days...\n",
      "upserting contract data for polygon_zkevm. Total rows: 15938...\n",
      "aggregagting native_transfers for polygon_zkevm and last 300 days...\n",
      "upserting native_transfers for polygon_zkevm. Total rows: 116...\n",
      "aggregagting sub categories for polygon_zkevm and last 300 days...\n",
      "upserting sub categories for polygon_zkevm. Total rows: 578...\n",
      "aggregagting unlabeled usage for polygon_zkevm and last 300 days...\n",
      "upserting unlabeled usage for polygon_zkevm. Total rows: 119...\n"
     ]
    }
   ],
   "source": [
    "for chain in chain_list:\n",
    "    ## aggregate contract data\n",
    "    print(f\"aggregagting contract data for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_contracts(chain, days)\n",
    "    df.set_index(['address', 'date', 'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting contract data for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_contract_level', df)\n",
    "\n",
    "    ## aggregate native transfers\n",
    "    print(f\"aggregagting native_transfers for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_native_transfers(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting native_transfers for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "\n",
    "    ## ALL below needs to be retriggerd when mapping changes (e.g. new addresses got labeled or new categories added etc.)\n",
    "    ## aggregate by sub categories\n",
    "    print(f\"aggregagting sub categories for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_sub_categories(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting sub categories for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    ## determine unlabeled usage\n",
    "    print(f\"aggregagting unlabeled usage for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_unlabeled(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting unlabeled usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to s3 bucket and output list of files\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket('gtp-longterm')\n",
    "\n",
    "## get list of files in bucket with last modified date\n",
    "files = []\n",
    "for obj in bucket.objects.all():\n",
    "    files.append([obj.key, obj.last_modified])\n",
    "\n",
    "df = pd.DataFrame(files, columns=['key', 'last_modified'])\n",
    "\n",
    "## filter out files where key starts with 'imx'\n",
    "df = df[~df.key.str.startswith('imx')]\n",
    "\n",
    "df['chain'] = df.key.str.split('/').str[0]\n",
    "\n",
    "## create new column block_range that extracts the string between 'tx_' and '.parquet' in the key column using lambda function\n",
    "df['block_range'] = df.key.apply(lambda x: x[x.find('tx_')+3:x.find('.parquet')])\n",
    "df['block_start'] = df.block_range.str.split('-').str[0].astype(int)\n",
    "df['block_end'] = df.block_range.str.split('-').str[1].astype(int)\n",
    "\n",
    "## sort by block_start\n",
    "df.sort_values(by='block_start', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum = df[df.chain == 'arbitrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_arbitrum_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum.key.iloc[0]}\")\n",
    "df_arbitrum_flipside.sort_values(by='ETH_VALUE', inplace=True, ascending=False)\n",
    "df_arbitrum_flipside.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_arbitrum_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum[df_arbitrum.block_start >= 96528665].key.iloc[10]}\")\n",
    "df_arbitrum_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism = df[df.chain == 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_optimism_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism.key.iloc[0]}\")\n",
    "df_optimism_flipside.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_optimism_flipside[['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED',\n",
    "#        'L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]\n",
    "\n",
    "## filter df_optimism_flipside where TX_HASH = 0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559 and select columns 'TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' \n",
    "\n",
    "df_optimism_flipside[df_optimism_flipside.TX_HASH == '0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559'.lower()][['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside['GAS_PRICE'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_optimism_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism[df_optimism.block_start >= 103428989].key.iloc[0]}\")\n",
    "df_optimism_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polygon zkEVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polygon = df[df.chain == 'polygon_zkevm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_polygon_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_polygon.key.iloc[0]}\")\n",
    "df_polygon_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zkSync Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zksync = df[df.chain == 'zksync_era']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_zksync_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_zksync.key.iloc[0]}\")\n",
    "df_zksync_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## rename files in S3 bucket that contain 'block_'\n",
    "# for index, row in df[df.key.str.contains('block_')].iterrows():\n",
    "#     print(row['key'])\n",
    "#     old_key = row['key']\n",
    "#     new_key = old_key.replace('block_', '')\n",
    "#     print(new_key)\n",
    "#     s3.Object('gtp-longterm', new_key).copy_from(CopySource='gtp-longterm/'+old_key)\n",
    "#     #s3.Object('gtp-longterm', old_key).delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum[df_arbitrum.block_start >= 96528665].sort_values(by='last_modified', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load polygon zkevm data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine, exc\n",
    "import sys\n",
    "from pangres import upsert\n",
    "\n",
    "# Load environment variables\n",
    "def load_environment():\n",
    "    load_dotenv()\n",
    "\n",
    "    # Postgres details from .env file\n",
    "    db_user = os.getenv(\"DB_USERNAME\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_database = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "    return db_user, db_password, db_host, db_database\n",
    "\n",
    "def connect_to_s3():\n",
    "    try:\n",
    "        aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "        aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        bucket_name = 'gtp-longterm'\n",
    "\n",
    "        if not aws_access_key_id or not aws_secret_access_key or not bucket_name:\n",
    "            raise ValueError(\"AWS access key ID, secret access key, or bucket name not found in environment variables.\")\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        )\n",
    "        return s3, bucket_name\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while connecting to S3:\", str(e))\n",
    "        return None, None\n",
    "\n",
    "def get_files_from_bucket(s3, bucket_name, folder_key):\n",
    "    return s3.list_objects(Bucket=bucket_name, Prefix=folder_key)\n",
    "\n",
    "def load_parquet_from_s3(s3, bucket_name, file_key):\n",
    "    print(f\"Loading file {file_key} from bucket {bucket_name}...\")\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        df = pd.read_parquet(BytesIO(obj['Body'].read()))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file {file_key} from bucket {bucket_name}.\")\n",
    "        print(e)\n",
    "\n",
    "def create_db_engine(db_user, db_password, db_host, db_database):\n",
    "    print(\"Creating database engine...\")\n",
    "    try:\n",
    "        # create connection to Postgres\n",
    "        engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}/{db_database}')\n",
    "        engine.connect()  # test connection\n",
    "        return engine\n",
    "    except exc.SQLAlchemyError as e:\n",
    "        print(\"Error connecting to database. Check your database configurations.\")\n",
    "        print(e)\n",
    "        raise ValueError(\"Error inserting data into table.\")\n",
    "\n",
    "def insert_into_db(df, engine, table_name):\n",
    "    # Set the DataFrame's index to 'tx_hash' (primary key)\n",
    "    df.set_index('tx_hash', inplace=True)\n",
    "    df.index.name = 'tx_hash'\n",
    "    \n",
    "    # Insert data into database\n",
    "    print(f\"Inserting data into table {table_name}...\")\n",
    "    try:\n",
    "        upsert(engine, df, table_name, if_row_exists='ignore')\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into table {table_name}.\")\n",
    "        print(e)\n",
    "        print(df)\n",
    "        raise ValueError(\"Error inserting data into table.\")\n",
    "\n",
    "def process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, start_index):\n",
    "    # if start_index is less than 0 or greater than the number of files, set it to 0\n",
    "    start_index = max(0, min(start_index, len(files_in_bucket['Contents']) - 1))\n",
    "    #for file in files_in_bucket['Contents'][:10]:  # Only process first 10 files\n",
    "    for file in files_in_bucket['Contents'][start_index:]:\n",
    "        # Print file index in the list\n",
    "        print(f\"--> Processing file {files_in_bucket['Contents'].index(file) + 1} of {len(files_in_bucket['Contents'])}...\")\n",
    "        if file['Key'].endswith('.parquet'):  # to ensure only .parquet files are processed\n",
    "            # Parse the min and max block numbers from the filename\n",
    "            filename = os.path.basename(file['Key'])\n",
    "            min_block, max_block = filename.rsplit('_', 1)[1].split('-')[0], filename.rsplit('_', 1)[1].split('-')[1].split('.')[0]\n",
    "            print(f\"--> Processing file {filename} with block range {min_block} - {max_block}\")\n",
    "\n",
    "            df = load_parquet_from_s3(s3, bucket_name, file['Key'])\n",
    "            if df is not None:\n",
    "                print(f\"Preparing DataFrame for file {filename}...\")\n",
    "                if table_name == 'polygon_zkevm_tx':\n",
    "                    df = prepare_dataframe_polygon(df)  # call to the function that prepares the dataframe\n",
    "                elif table_name == 'zksync_era_tx':\n",
    "                    df = prepare_dataframe_zksync(df)  # call to the function that prepares the dataframe\n",
    "                insert_into_db(df, engine, table_name)\n",
    "\n",
    "def prepare_dataframe_polygon(df):\n",
    "    print(\"Preparing DataFrame for polygon_zkevm_tx...\")\n",
    "    # Columns to be used from the dataframe\n",
    "    cols = ['block_number', 'block_time', 'hash', 'from_address', 'to_address', 'status', 'value', 'gas_limit', 'gas_used', 'gas_price', 'type', 'receipt_contract_address']\n",
    "\n",
    "    # Filter the dataframe to only include the above columns plus 'input' for calculations\n",
    "    df = df.loc[:, cols + ['input']]\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'hash': \"tx_hash\", \"block_time\":\"block_timestamp\"}, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Add tx_fee column\n",
    "    df['tx_fee'] = df['gas_used'] * df['gas_price']  / 1e18\n",
    "\n",
    "    # gas_price column: convert to eth\n",
    "    df['gas_price'] = df['gas_price'].astype(float) / 1e18\n",
    "\n",
    "    # Add native_transfer column True when (input = 0x OR input is empty) and value > 0 then true else false\n",
    "    df['value'] = df['value'].astype(float) / 1e18\n",
    "\n",
    "    # Add empty_input column True when input is empty or 0x then true else false\n",
    "    df['empty_input'] = df['input'].apply(lambda x: True if (x == '0x' or x == '') else False)\n",
    "\n",
    "    # status column: 1 if status is success, 0 if failed else -1\n",
    "    df['status'] = df['status'].apply(lambda x: 1 if x == 1 else 0 if x == 0 else -1)\n",
    "\n",
    "    # Convert block_time to datetime\n",
    "    df['block_timestamp'] = pd.to_datetime(df['block_timestamp'])\n",
    "\n",
    "    # Handle bytea data type\n",
    "    for col in ['tx_hash', 'to_address', 'from_address', 'receipt_contract_address']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.replace('0x', '\\\\x', regex=False)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in dataframe.\")\n",
    "    \n",
    "    # Drop the 'input' column\n",
    "    df = df.drop(columns=['input'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe_zksync(df):\n",
    "    print(\"Preparing DataFrame for zksync_era_tx...\")\n",
    "\n",
    "    # Columns to be used from the dataframe\n",
    "    cols = ['block_number', 'block_time', 'hash', 'from_address', 'to_address', 'status', 'value', 'gas_limit', 'gas_used', 'gas_price', 'type', 'receipt_contract_address']\n",
    "\n",
    "    # Filter the dataframe to only include the above columns plus 'input' for calculations\n",
    "    df = df.loc[:, cols + ['input']]\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'hash': \"tx_hash\", \"block_time\":\"block_timestamp\"}, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Add tx_fee column\n",
    "    df['tx_fee'] = df['gas_used'] * df['gas_price']  / 1e18\n",
    "\n",
    "    # gas_price column: convert to eth\n",
    "    df['gas_price'] = df['gas_price'].astype(float) / 1e18\n",
    "\n",
    "    # value column divide by 1e18 to convert to eth\n",
    "    df['value'] = df['value'].astype(float) / 1e18\n",
    "\n",
    "    # Add empty_input column True when input is empty or 0x then true else false\n",
    "    df['empty_input'] = df['input'].apply(lambda x: True if (x == '0x' or x == '') else False)\n",
    "\n",
    "    # status column: 1 if status is success, 0 if failed else -1\n",
    "    df['status'] = df['status'].apply(lambda x: 1 if x == 1 else 0 if x == 0 else -1)\n",
    "\n",
    "    # Convert block_time to datetime\n",
    "    df['block_timestamp'] = pd.to_datetime(df['block_timestamp'])\n",
    "\n",
    "    # Handle bytea data type\n",
    "    for col in ['tx_hash', 'to_address', 'from_address', 'receipt_contract_address']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('0x', '\\\\x', regex=False)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in dataframe.\")\n",
    "    \n",
    "    # Drop the 'input' column\n",
    "    df = df.drop(columns=['input'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main_polygon():\n",
    "    # List of folders in the S3 bucket\n",
    "    chain_folders = ['arbitrum', 'imx', 'optimism', 'polygon_zkevm', 'zksync_era']\n",
    "    \n",
    "    # Load environment variables\n",
    "    db_user, db_password, db_host, db_database = load_environment()\n",
    "\n",
    "    # Create DB engine\n",
    "    engine = create_db_engine(db_user, db_password, db_host, db_database)\n",
    "\n",
    "    # Connect to S3 bucket\n",
    "    s3, bucket_name = connect_to_s3()\n",
    "\n",
    "    # Select the desired chain folder\n",
    "    folder_key = chain_folders[3]+'/'\n",
    "\n",
    "    # Get list of all files in the specific S3 bucket and folder\n",
    "    files_in_bucket = get_files_from_bucket(s3, bucket_name, folder_key)\n",
    "\n",
    "    # Sort the files by min_block\n",
    "    files_in_bucket['Contents'].sort(key=lambda x: int(os.path.basename(x['Key']).rsplit('_', 1)[1].split('-')[0]))\n",
    "\n",
    "    # Table name in the database\n",
    "    table_name = 'polygon_zkevm_tx'\n",
    "\n",
    "    # Process the files in the bucket\n",
    "    process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, 0)\n",
    "\n",
    "def main_zksync():\n",
    "    # List of folders in the S3 bucket\n",
    "    chain_folders = ['arbitrum', 'imx', 'optimism', 'polygon_zkevm', 'zksync_era']\n",
    "    \n",
    "    # Load environment variables\n",
    "    db_user, db_password, db_host, db_database = load_environment()\n",
    "\n",
    "    # Create DB engine\n",
    "    engine = create_db_engine(db_user, db_password, db_host, db_database)\n",
    "\n",
    "    # Connect to S3 bucket\n",
    "    s3, bucket_name = connect_to_s3()\n",
    "\n",
    "    # Select the desired chain folder\n",
    "    folder_key = chain_folders[4]+'/'\n",
    "\n",
    "    # Get list of all files in the specific S3 bucket and folder\n",
    "    files_in_bucket = get_files_from_bucket(s3, bucket_name, folder_key)\n",
    "\n",
    "    # Sort the files by min_block\n",
    "    files_in_bucket['Contents'].sort(key=lambda x: int(os.path.basename(x['Key']).rsplit('_', 1)[1].split('-')[0]))\n",
    "\n",
    "    # Table name in the database\n",
    "    table_name = 'zksync_era_tx'\n",
    "\n",
    "    # Process the files in the bucket\n",
    "    process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, 981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating database engine...\n",
      "--> Processing file 982 of 982...\n",
      "--> Processing file zksync_era_tx_8990698-9000813.parquet with block range 8990698 - 9000813\n",
      "Loading file zksync_era/zksync_era_tx_8990698-9000813.parquet from bucket gtp-longterm...\n",
      "Preparing DataFrame for file zksync_era_tx_8990698-9000813.parquet...\n",
      "Preparing DataFrame for zksync_era_tx...\n",
      "Inserting data into table zksync_era_tx...\n",
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "main_zksync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
