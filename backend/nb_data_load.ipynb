{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.misc.helper_functions import api_get_call\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "base_url = 'https://l2beat.com/api/'\n",
    "\n",
    "\n",
    "origin_keys = ['arbitrum', 'optimism', 'base', 'zksync-era', 'dydx', 'starknet', 'mantle', 'immutablex', 'loopring', 'linea', 'zksync-lite', 'metis', 'polygonzkevm', 'apex', 'nova', 'zkspace', 'sorare', 'rhinofi', 'mantapacific',\n",
    "               'bobanetwork', 'aevo', 'zora', 'aztecconnect', 'degate2', 'aztec', 'scroll', 'brine', 'publicgoodsnetwork', 'kroma', 'myria', 'canvasconnect', 'cartesi-honeypot', 'fuelv1']\n",
    "\n",
    "dfMain = pd.DataFrame()\n",
    "for origin_key in origin_keys:\n",
    "    url = f\"{base_url}/tvl/{origin_key}.json\"           \n",
    "\n",
    "    response_json = api_get_call(url, sleeper=10, retries=20)\n",
    "    df = pd.json_normalize(response_json['daily'], record_path=['data'], sep='_')\n",
    "\n",
    "    ## only keep the columns 0 (date) and 1 (total tvl)\n",
    "    df = df.iloc[:,[0,1]]\n",
    "\n",
    "    df['date'] = pd.to_datetime(df[0],unit='s')\n",
    "    df['date'] = df['date'].dt.date\n",
    "    df.drop(df[df[1] == 0].index, inplace=True)\n",
    "    df.drop([0], axis=1, inplace=True)\n",
    "    df.rename(columns={1:'value'}, inplace=True)\n",
    "    df['metric_key'] = 'tvl'\n",
    "    df['origin_key'] = origin_key\n",
    "    max_date = df['date'].max()\n",
    "    df.drop(df[df.date == max_date].index, inplace=True)\n",
    "    today = datetime.today().strftime('%Y-%m-%d')\n",
    "    df.drop(df[df.date == today].index, inplace=True, errors='ignore')\n",
    "    df.value.fillna(0, inplace=True)\n",
    "    dfMain = pd.concat([dfMain,df])\n",
    "\n",
    "    print(f\"...loaded for {origin_key}. Shape: {df.shape}\")\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMain.to_csv('tvl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only keep first value of each month\n",
    "dfMain['month'] = pd.DatetimeIndex(dfMain['date']).month\n",
    "dfMain['year'] = pd.DatetimeIndex(dfMain['date']).year\n",
    "dfMain['day'] = pd.DatetimeIndex(dfMain['date']).day\n",
    "dfMain['first_of_month'] = dfMain['day'] == 1\n",
    "dfMain = dfMain[dfMain['first_of_month'] == True]\n",
    "dfMain.drop(['month', 'year', 'day', 'first_of_month'], axis=1, inplace=True)\n",
    "dfMain.to_csv('tvl_first_of_month.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpivot origin_key column\n",
    "test = dfMain.pivot(index='origin_key', columns='date', values='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test df to csv\n",
    "test.to_csv('test.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_val = -700\n",
    "cur_val = -200\n",
    "\n",
    "change_val = (prev_val - cur_val) / prev_val\n",
    "change_val = round(change_val, 4)\n",
    "\n",
    "print(change_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### api.growthepie.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://api.growthepie.xyz/v1/fundamentals.json'\n",
    "response = requests.get(url)\n",
    "df = pd.DataFrame(response.json())\n",
    "\n",
    "df.head(8)\n",
    "\n",
    "## filter df by metric_key == 'txcount' and origin_key == 'base'\n",
    "df = df[(df['metric_key'] == 'txcount') & (df['origin_key'] == 'base')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot metric txcount over date for all origin_keys and order by date\n",
    "df[(df['metric_key'] == 'txcount') & (df['origin_key'] == 'arbitrum')].sort_values('date').plot(x='date', y='value', figsize=(15, 5), title='Arbitrum Daily Transactions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airtable labelling help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AIRTABLE\n",
    "import pandas as pd\n",
    "import airtable\n",
    "import os\n",
    "\n",
    "#initialize Airtable instance\n",
    "AIRTABLE_API_KEY = os.getenv(\"AIRTABLE_API_KEY\")\n",
    "AIRTABLE_BASE_ID = os.getenv(\"AIRTABLE_BASE_ID\")\n",
    "at = airtable.Airtable(AIRTABLE_BASE_ID, AIRTABLE_API_KEY)\n",
    "\n",
    "data = pd.DataFrame([c['fields'] for c in at.get('Unlabeled Contracts')['records']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "import src.misc.airtable_functions as at\n",
    "from eth_utils import to_checksum_address\n",
    "\n",
    "def read_airtable():\n",
    "    # read current airtable\n",
    "    df = at.read_all_airtable()\n",
    "    if df is None:\n",
    "        print(\"Nothing to upload\")\n",
    "    else:\n",
    "        df['added_on_time'] = datetime.now()\n",
    "        df.set_index(['address', 'origin_key'], inplace=True)\n",
    "        # initialize db connection\n",
    "        db_connector = DbConnector()\n",
    "        db_connector.upsert_table('blockspace_labels' ,df)\n",
    "\n",
    "\n",
    "def write_airtable():\n",
    "    # delete every row in airtable\n",
    "    at.clear_all_airtable()\n",
    "    # initialize db connection\n",
    "    db_connector = DbConnector()\n",
    "    # get top unlabelled contracts\n",
    "    df = db_connector.get_unlabelled_contracts('15', '30')\n",
    "    df['address'] = df['address'].apply(lambda x: to_checksum_address('0x' + bytes(x).hex()))\n",
    "    # write to airtable\n",
    "    at.push_to_airtable(df)\n",
    "\n",
    "read_airtable()\n",
    "write_airtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web3 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "import os\n",
    "\n",
    "tx = '0xdba1ec7832bc857a4f3624557572f24169dda03501fd2b6d6a529647c9a64c51'\n",
    "url = \"https://rpc.zora.energy/\"\n",
    "#url = f\"https://rpc.ankr.com/base/{os.getenv('ANKR_API')}\"\n",
    "w3 = Web3(Web3.HTTPProvider(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = w3.eth.get_block(3913960, full_transactions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt = w3.eth.get_transaction_receipt(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.misc.helper_functions import api_post_call\n",
    "import json\n",
    " \n",
    "payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getTransactionReceipt\",\n",
    "        \"params\": [tx],\n",
    "        \"id\": 1\n",
    "    }\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = api_post_call(url, payload=json.dumps(payload), header=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_number_hex = hex(3913960)\n",
    "payload = {\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"eth_getBlockByNumber\",\n",
    "    \"params\": [str(block_number_hex), True],\n",
    "    \"id\": 1\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = api_post_call(url, payload=json.dumps(payload), header=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "from src.adapters.adapter_raw_rpc import AdapterRPCRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'ankr',\n",
    "    'api_key' : os.getenv(\"ANKR_API\"),\n",
    "    'chain' : 'base'\n",
    "}\n",
    "load_params = {\n",
    "    'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    #'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "    'batch_size' : 25,\n",
    "    'threads' : 1\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "db_connector = DbConnector()\n",
    "# initialize adapter\n",
    "ad = AdapterRPCRaw(adapter_params, db_connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.api.json_creation import JSONCreation\n",
    "from src.db_connector import DbConnector\n",
    "\n",
    "db_connector = DbConnector()\n",
    "\n",
    "json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"v1\")\n",
    "## for testing\n",
    "#json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"test\")\n",
    "\n",
    "#json_creator.create_all_jsons()\n",
    "\n",
    "df = json_creator.get_all_data()\n",
    "#json_creator.create_landingpage_json(df)\n",
    "#json_creator.create_chain_details_jsons(df)\n",
    "#json_creator.create_metric_details_jsons(df)\n",
    "#json_creator.create_fundamentals_json(df)\n",
    "#json_creator.create_master_json()\n",
    "#json_creator.create_contracts_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "from src.api.blockspace_json_creation import BlockspaceJSONCreation\n",
    "db_connector = DbConnector()\n",
    "api_version = \"v1\"\n",
    "\n",
    "blockspace_json_creator = BlockspaceJSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, api_version)\n",
    "#blockspace_json_creator.create_blockspace_overview_json()\n",
    "blockspace_json_creator.create_blockspace_comparison_json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_l2beat import AdapterL2Beat\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['arbitrum'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterL2Beat(adapter_params, db_connector)\n",
    "# extract\n",
    "df= ad.extract(load_params)\n",
    "# load\n",
    "#ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefiLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_defillama import AdapterDefiLlama\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['ethereum'] # see all options in adapter_mapping.py\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDefiLlama(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coingecko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for projects / chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'project',\n",
    "    'metric_keys' : ['price', 'volume', 'market_cap'],\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['aptos'], # see all options in adapter_mapping.py\n",
    "    'days' : 'auto', # auto, max, or a number (as string)\n",
    "    'vs_currencies' : ['usd', 'eth']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for imx tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'imx_tokens'\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'query_names' : ['rent_paid'], ## fundamentals, waa, stables_mcap, rent_paid\n",
    "    'days' : 30,\n",
    "    #'query_names' : None,\n",
    "    #'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "#ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside\n",
    "sometimes some Flipside queries just get stuck -- gotta retrigger them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_flipside import AdapterFlipside\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : ['zksync_era'],\n",
    "    'metric_keys' : ['stables_mcap'],\n",
    "    'days' : 'auto',\n",
    "    # 'origin_keys' : None,\n",
    "    # 'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipside(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the new flipside api\n",
    "\n",
    "from flipside import Flipside\n",
    "flipside = Flipside(\"74c43ebc-3291-4953-8aeb-65640da7c852\", \"https://api-v2.flipsidecrypto.xyz\")\n",
    "\n",
    "sql = \"\"\"\n",
    " select \n",
    "    BLOCK_NUMBER, BLOCK_TIMESTAMP, BLOCK_HASH, TX_HASH, NONCE, POSITION, ORIGIN_FUNCTION_SIGNATURE, FROM_ADDRESS, TO_ADDRESS, ETH_VALUE, TX_FEE, GAS_PRICE, GAS_LIMIT, \n",
    "    GAS_USED, L1_GAS_PRICE, L1_GAS_USED, L1_FEE_SCALAR, L1_SUBMISSION_BATCH_INDEX, L1_SUBMISSION_TX_HASH, L1_STATE_ROOT_BATCH_INDEX, \n",
    "    L1_STATE_ROOT_TX_HASH, CUMULATIVE_GAS_USED, INPUT_DATA, STATUS\n",
    "from optimism.core.fact_transactions\n",
    "where block_number >= 104130000\n",
    "and block_number < 104135000\n",
    "order by block_number asc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query against Flipside's query engine and await the results\n",
    "query_result_set = flipside.query(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zettablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_zettablock import AdapterZettablock\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : ['zksync_era', 'polygon_zkevm'],\n",
    "    #'metric_keys' : ['txcount', ''],\n",
    "    'days' : 10,\n",
    "    'origin_keys' : None,\n",
    "    'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettablock(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPC raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_rpc import AdapterRPCRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'ankr',\n",
    "    'api_key' : os.getenv(\"ANKR_API\"),\n",
    "    'chain' : 'base'\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    #'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "    'batch_size' : 10,\n",
    "    'threads' : 5\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRPCRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_rpc import AdapterRPCRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'ankr',\n",
    "    'api_key' : os.getenv(\"ANKR_API\"),\n",
    "    'chain' : 'optimism'\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    #'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "    'batch_size' : 10,\n",
    "    'threads' : 5\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRPCRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter Nader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.adapters.adapter_nader import BaseNodeAdapter\n",
    "from src.db_connector import DbConnector\n",
    "adapter_params = {\n",
    "    'rpc': 'local_node',\n",
    "    'chain': 'base',\n",
    "    'node_url': os.getenv(\"BASE_NODE\"),\n",
    "}\n",
    "\n",
    "# Initialize DbConnector\n",
    "db_connector = DbConnector()\n",
    "\n",
    "# Initialize BaseNodeAdapter\n",
    "adapter = BaseNodeAdapter(adapter_params, db_connector)\n",
    "\n",
    "# Test database connectivity\n",
    "if not adapter.check_db_connection():\n",
    "    print(\"Failed to connect to database.\")\n",
    "else:\n",
    "    print(\"Successfully connected to database.\")\n",
    "\n",
    "# Test S3 connectivity\n",
    "if not adapter.check_s3_connection():\n",
    "    print(\"Failed to connect to S3.\")\n",
    "else:\n",
    "    print(\"Successfully connected to S3.\")\n",
    "\n",
    "# Test run method\n",
    "load_params = {\n",
    "    'block_start': 'auto',\n",
    "    'batch_size': 10,\n",
    "    'threads': 5,\n",
    "}\n",
    "\n",
    "adapter.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZettaBlock raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_zettablock import AdapterZettaBlockRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    #'keys' : ['polygon_zkevm_tx', 'zksync_era_tx'],\n",
    "    'keys' : ['zksync_era_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettaBlockRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params, if_exists = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainbase raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_chainbase import AdapterChainbaseRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"CHAINBASE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 64900000, ## until 65,570,000\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterChainbaseRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMX raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement orchestration?\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_imx import AdapterRawImx\n",
    "\n",
    "adapter_params = {\n",
    "    'load_types' : ['withdrawals', 'deposits', 'trades', 'orders_filled', 'transfers', 'mints'],\n",
    "    'forced_refresh' : 'no',\n",
    "\n",
    "    #'load_types' : ['orders_filled'],\n",
    "    #'forced_refresh' : '2023-04-01T00:00:00.000000Z',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRawImx(adapter_params, db_connector)\n",
    "# extract raw (and load raw in case of IMX)\n",
    "df_raw = ad.extract_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_flipside import AdapterFlipsideRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx', 'optimism_tx'],\n",
    "    'block_start' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipsideRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to orbal_writer@orbal-main.cydw1x28knil.us-east-1.rds.amazonaws.com:5432\n",
      "Adapter SQL Aggregation initialized with {}.\n",
      "... executing query: user_base_weekly - multi with {'Days': 200, 'aggregation': 'week'} days\n",
      "...query loaded: user_base_weekly multi with 200 days. DF shape: (252, 4)\n",
      "SQL Aggregation extract done for {'load_type': 'metrics', 'origin_keys': None, 'days': 200, 'metric_keys': ['user_base_weekly']}. DataFrame shape: (252, 1)\n",
      "Load SQL Aggregation done - 252 rows upserted in fact_kpis\n"
     ]
    }
   ],
   "source": [
    "## Loads currently IMX txcount, daa, fees_paid\n",
    "## also loads user_base_weekly\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics', ## usd_to_eth or metrics or blockspace\n",
    "    \n",
    "    #'days' : 'auto', ## days as int our 'auto\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    #'metric_keys' : None, ## metric_keys as list or None\n",
    "\n",
    "    'days' : 200, ## days as int our 'auto\n",
    "    #'origin_keys' : ['zora', 'gitcoin_pgn'], ## origin_keys as list or None\n",
    "    'metric_keys' : ['user_base_weekly'], ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'profit', ## usd_to_eth or metrics or blockspace or profit\n",
    "    'days' : 30, ## days as int\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'usd_to_eth', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : 30, ## days as int\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to orbal_writer@orbal-main.cydw1x28knil.us-east-1.rds.amazonaws.com:5432\n",
      "Adapter SQL Aggregation initialized with {}.\n",
      "...aggregating contract data for linea and last 300 days...\n",
      "...upserting contract data for linea. Total rows: 71125...\n",
      "...aggregating total usage for linea and last 300 days...\n",
      "...upserting total usage usage for linea. Total rows: 128...\n",
      "...aggregating native_transfers for linea and last 300 days...\n",
      "...upserting native_transfers for linea. Total rows: 127...\n",
      "...aggregating smart_contract_deployments for linea and last 300 days...\n",
      "...upserting smart_contract_deployments for linea. Total rows: 127...\n",
      "...aggregating sub categories for linea and last 5000 days...\n",
      "...upserting sub categories for linea. Total rows: 0...\n",
      "...aggregating unlabeled usage for linea and last 5000 days...\n",
      "...upserting unlabeled usage for linea. Total rows: 128...\n",
      "Finished loading blockspace queries for linea\n",
      "Finished loading blockspace for all chains\n"
     ]
    }
   ],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'blockspace', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : '300', ## days as or auto\n",
    "    #'origin_keys' : ['arbitrum', 'zksync_era', 'polygon_zkevm', 'optimism', 'base', 'imx'], ## origin_keys as list or None\n",
    "    'origin_keys' : ['linea']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "ad.extract(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockspace logic\n",
    "- for each chain, aggregate the daily contracts usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "from src.misc.helper_functions import get_missing_days_blockspace\n",
    "db_connector = DbConnector()\n",
    "\n",
    "chain_list = ['optimism']\n",
    "#chain_list = ['arbitrum', 'zksync_era', 'polygon_zkevm', 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chain in chain_list:\n",
    "    #days = get_missing_days_blockspace(db_connector, chain)\n",
    "    days = 10000\n",
    "\n",
    "    # ## aggregate contract data\n",
    "    # print(f\"aggregating contract data for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_contracts(chain, days)\n",
    "    # df.set_index(['address', 'date', 'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting contract data for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_contract_level', df)\n",
    "\n",
    "    # ## determine total usage\n",
    "    # print(f\"aggregating total usage for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_total(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting total usage usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # ## aggregate native transfers\n",
    "    # print(f\"aggregating native_transfers for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_native_transfers(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting native_transfers for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # ## aggregate contract deployments\n",
    "    # print(f\"aggregating smart_contract_deployments for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_contract_deplyments(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting smart_contract_deployments for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # ALL below needs to be retriggerd when mapping changes (e.g. new addresses got labeled or new categories added etc.)\n",
    "    ## aggregate by sub categories\n",
    "    print(f\"aggregating sub categories for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_sub_categories(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting sub categories for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    ## determine unlabeled usage\n",
    "    print(f\"aggregating unlabeled usage for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_unlabeled(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting unlabeled usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "\n",
    "# days = get_missing_days_blockspace(db_connector, 'imx')\n",
    "\n",
    "# df = db_connector.get_blockspace_imx(days)\n",
    "# df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "# print(f\"...upserting imx data . Total rows: {df.shape[0]}...\")\n",
    "# db_connector.upsert_table('blockspace_fact_sub_category_level', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_connector.get_top_contracts_by_category('main_category', 'unlabeled', 'arbitrum', 'gas', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to s3 bucket and output list of files\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket('gtp-longterm')\n",
    "\n",
    "## get list of files in bucket with last modified date\n",
    "files = []\n",
    "for obj in bucket.objects.all():\n",
    "    files.append([obj.key, obj.last_modified])\n",
    "\n",
    "df = pd.DataFrame(files, columns=['key', 'last_modified'])\n",
    "\n",
    "## filter out files where key starts with 'imx'\n",
    "df = df[~df.key.str.startswith('imx')]\n",
    "\n",
    "df['chain'] = df.key.str.split('/').str[0]\n",
    "\n",
    "## create new column block_range that extracts the string between 'tx_' and '.parquet' in the key column using lambda function\n",
    "df['block_range'] = df.key.apply(lambda x: x[x.find('tx_')+3:x.find('.parquet')])\n",
    "df['block_start'] = df.block_range.str.split('-').str[0].astype(int)\n",
    "df['block_end'] = df.block_range.str.split('-').str[1].astype(int)\n",
    "\n",
    "## sort by block_start\n",
    "df.sort_values(by='block_start', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum = df[df.chain == 'arbitrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_arbitrum_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum.key.iloc[0]}\")\n",
    "df_arbitrum_flipside.sort_values(by='ETH_VALUE', inplace=True, ascending=False)\n",
    "df_arbitrum_flipside.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_arbitrum_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum[df_arbitrum.block_start >= 96528665].key.iloc[10]}\")\n",
    "df_arbitrum_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism = df[df.chain == 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_optimism_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism.key.iloc[0]}\")\n",
    "df_optimism_flipside.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_optimism_flipside[['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED',\n",
    "#        'L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]\n",
    "\n",
    "## filter df_optimism_flipside where TX_HASH = 0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559 and select columns 'TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' \n",
    "\n",
    "df_optimism_flipside[df_optimism_flipside.TX_HASH == '0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559'.lower()][['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside['GAS_PRICE'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_optimism_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism[df_optimism.block_start >= 103428989].key.iloc[0]}\")\n",
    "df_optimism_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polygon zkEVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polygon = df[df.chain == 'polygon_zkevm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_polygon_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_polygon.key.iloc[0]}\")\n",
    "df_polygon_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zkSync Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zksync = df[df.chain == 'zksync_era']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_zksync_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_zksync.key.iloc[0]}\")\n",
    "df_zksync_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## rename files in S3 bucket that contain 'block_'\n",
    "# for index, row in df[df.key.str.contains('block_')].iterrows():\n",
    "#     print(row['key'])\n",
    "#     old_key = row['key']\n",
    "#     new_key = old_key.replace('block_', '')\n",
    "#     print(new_key)\n",
    "#     s3.Object('gtp-longterm', new_key).copy_from(CopySource='gtp-longterm/'+old_key)\n",
    "#     #s3.Object('gtp-longterm', old_key).delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum[df_arbitrum.block_start >= 96528665].sort_values(by='last_modified', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_zksync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexed.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "aws_access_key_id = \"43c31ff797ec2387177cabab6d18f15a\"\n",
    "aws_secret_access_key = \"afb354f05026f2512557922974e9dd2fdb21e5c2f5cbf929b35f0645fb284cf7\"\n",
    "bucket_name = 'indexed-xyz'\n",
    "\n",
    "s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Set the profile name\n",
    "profile_name = 'indexedxyz'\n",
    "\n",
    "# Create an S3 client using the profile\n",
    "session = boto3.Session(profile_name=profile_name)\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test s3 connection\n",
    "response = s3.list_objects(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list all files in bucket\n",
    "for obj in s3.list_objects(Bucket=bucket_name)['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.adapters.adapter_raw_gtp import NodeAdapter\n",
    "from src.db_connector import DbConnector\n",
    "from datetime import datetime,timedelta\n",
    "from airflow.decorators import dag, task \n",
    "import os\n",
    "\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'local_node',\n",
    "    'chain': 'zora',\n",
    "    'node_url': os.getenv(\"ZORA_RPC\"),\n",
    "}\n",
    "\n",
    "# Initialize DbConnector\n",
    "db_connector = DbConnector()\n",
    "\n",
    "# Initialize NodeAdapter\n",
    "adapter = NodeAdapter(adapter_params, db_connector)\n",
    "\n",
    "# Test database connectivity\n",
    "if not adapter.check_db_connection():\n",
    "    print(\"Failed to connect to database.\")\n",
    "else:\n",
    "    print(\"Successfully connected to database.\")\n",
    "\n",
    "# Test S3 connectivity\n",
    "if not adapter.check_s3_connection():\n",
    "    print(\"Failed to connect to S3.\")\n",
    "else:\n",
    "    print(\"Successfully connected to S3.\")\n",
    "    \n",
    "# Extract\n",
    "load_params = {\n",
    "    'block_start': '4982120',\n",
    "    'batch_size': 1,\n",
    "    'threads': 1,\n",
    "}\n",
    "adapter.extract_raw(load_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
