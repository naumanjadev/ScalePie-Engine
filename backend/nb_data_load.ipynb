{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... uploaded to v1/master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.api.json_creation import JSONCreation\n",
    "from src.db_connector import DbConnector\n",
    "\n",
    "db_connector = DbConnector()\n",
    "\n",
    "json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"v1\")\n",
    "## for testing\n",
    "#json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"test\")\n",
    "\n",
    "#json_creator.create_all_jsons()\n",
    "\n",
    "df = json_creator.get_all_data()\n",
    "#json_creator.create_landingpage_json(df)\n",
    "#json_creator.create_chain_details_jsons(df)\n",
    "#json_creator.create_metric_details_jsons(df)\n",
    "json_creator.create_master_json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter L2Beat initialized with {}.\n",
      "...L2Beat - loaded for base. Shape: (55, 4)\n",
      "L2Beat extract done for {'origin_keys': ['base']}. DataFrame shape: (55, 1)\n",
      "Load L2Beat done - 55 rows upserted in fact_kpis\n"
     ]
    }
   ],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_l2beat import AdapterL2Beat\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : None,\n",
    "    'origin_keys' : ['base'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterL2Beat(adapter_params, db_connector)\n",
    "# extract\n",
    "df= ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefiLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_defillama import AdapterDefiLlama\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['ethereum'] # see all options in adapter_mapping.py\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDefiLlama(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coingecko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for projects / chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'project',\n",
    "    'metric_keys' : ['price', 'volume', 'market_cap'],\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['aptos'], # see all options in adapter_mapping.py\n",
    "    'days' : 'auto', # auto, max, or a number (as string)\n",
    "    'vs_currencies' : ['usd', 'eth']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for imx tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'imx_tokens'\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter Dune initialized with {'api_key': '***'}.\n",
      "...start loading fundamentals with query_id: 2607041 and params: [Parameter(name=Days, value=60, type=text)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 20:08:23,226 INFO dune_client.base_client waiting for query execution 01H7GA59DM9WQYTBRZV83TH1M1 to complete: ExecutionState.EXECUTING\n",
      "2023-08-10 20:08:28,373 INFO dune_client.base_client waiting for query execution 01H7GA59DM9WQYTBRZV83TH1M1 to complete: ExecutionState.EXECUTING\n",
      "2023-08-10 20:08:33,537 INFO dune_client.base_client waiting for query execution 01H7GA59DM9WQYTBRZV83TH1M1 to complete: ExecutionState.EXECUTING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...finished loading fundamentals. Loaded 908 rows\n",
      "Dune extract done for {'query_names': ['fundamentals'], 'days': 60}. DataFrame shape: (908, 1)\n",
      "Load Dune done - 908 rows upserted in fact_kpis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'query_names' : ['fundamentals'], ## fundamentals, waa, stables_mcap\n",
    "    'days' : 60,\n",
    "    #'query_names' : None,\n",
    "    #'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside\n",
    "sometimes some Flipside queries just get stuck -- gotta retrigger them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_flipside import AdapterFlipside\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : ['zksync_era'],\n",
    "    'metric_keys' : ['stables_mcap'],\n",
    "    'days' : 'auto',\n",
    "    # 'origin_keys' : None,\n",
    "    # 'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipside(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the new flipside api\n",
    "\n",
    "from flipside import Flipside\n",
    "flipside = Flipside(\"74c43ebc-3291-4953-8aeb-65640da7c852\", \"https://api-v2.flipsidecrypto.xyz\")\n",
    "\n",
    "sql = \"\"\"\n",
    " select \n",
    "    BLOCK_NUMBER, BLOCK_TIMESTAMP, BLOCK_HASH, TX_HASH, NONCE, POSITION, ORIGIN_FUNCTION_SIGNATURE, FROM_ADDRESS, TO_ADDRESS, ETH_VALUE, TX_FEE, GAS_PRICE, GAS_LIMIT, \n",
    "    GAS_USED, L1_GAS_PRICE, L1_GAS_USED, L1_FEE_SCALAR, L1_SUBMISSION_BATCH_INDEX, L1_SUBMISSION_TX_HASH, L1_STATE_ROOT_BATCH_INDEX, \n",
    "    L1_STATE_ROOT_TX_HASH, CUMULATIVE_GAS_USED, INPUT_DATA, STATUS\n",
    "from optimism.core.fact_transactions\n",
    "where block_number >= 104130000\n",
    "and block_number < 104135000\n",
    "order by block_number asc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query against Flipside's query engine and await the results\n",
    "query_result_set = flipside.query(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zettablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_zettablock import AdapterZettablock\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : ['zksync_era', 'polygon_zkevm'],\n",
    "    #'metric_keys' : ['txcount', ''],\n",
    "    'days' : 10,\n",
    "    'origin_keys' : None,\n",
    "    'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettablock(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZettaBlock raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_zettablock import AdapterZettaBlockRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    #'keys' : ['polygon_zkevm_tx', 'zksync_era_tx'],\n",
    "    'keys' : ['zksync_era_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettaBlockRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params, if_exists = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainbase raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_chainbase import AdapterChainbaseRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"CHAINBASE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 64900000, ## until 65,570,000\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterChainbaseRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMX raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement orchestration?\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_imx import AdapterRawImx\n",
    "\n",
    "adapter_params = {\n",
    "    'load_types' : ['withdrawals', 'deposits', 'trades', 'orders_filled', 'transfers', 'mints'],\n",
    "    'forced_refresh' : 'no',\n",
    "\n",
    "    #'load_types' : ['orders_filled'],\n",
    "    #'forced_refresh' : '2023-04-01T00:00:00.000000Z',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRawImx(adapter_params, db_connector)\n",
    "# extract raw (and load raw in case of IMX)\n",
    "df_raw = ad.extract_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_flipside import AdapterFlipsideRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx', 'optimism_tx'],\n",
    "    'block_start' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipsideRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads currently IMX txcount, daa, fees_paid\n",
    "## also loads user_base_weekly\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics', ## usd_to_eth or metrics or blockspace\n",
    "    \n",
    "    #'days' : 'auto', ## days as int our 'auto\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    #'metric_keys' : None, ## metric_keys as list or None\n",
    "\n",
    "    'days' : '28', ## days as int our 'auto\n",
    "    #'origin_keys' : ['imx'], ## origin_keys as list or None\n",
    "    'metric_keys' : ['user_base_weekly'], ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter SQL Aggregation initialized with {}.\n",
      "load usd values for : 'tvl', 'rent_paid_usd', 'fees_paid_usd', 'stables_mcap', 'txcosts_median_usd'\n",
      "SQL Aggregation extract done for {'load_type': 'usd_to_eth', 'days': 100, 'origin_keys': None, 'metric_keys': None}. DataFrame shape: (2362, 1)\n",
      "Load SQL Aggregation done - 2362 rows upserted in fact_kpis\n"
     ]
    }
   ],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'usd_to_eth', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : 100, ## days as int\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter SQL Aggregation initialized with {}.\n",
      "Last blockspace entry for origin_key: imx is on 2023-08-02. Set days to 4.\n",
      "...aggregating imx data for last 4 days...\n",
      "...upserting imx data . Total rows: 20...\n",
      "Finished loading blockspace queries for imx\n",
      "Finished loading blockspace for all chains\n"
     ]
    }
   ],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'blockspace', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : 'auto', ## days as or auto\n",
    "    #'origin_keys' : ['arbitrum', 'zksync_era', 'polygon_zkevm', 'imx'], ## origin_keys as list or None\n",
    "    'origin_keys' : ['imx']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "ad.extract(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockspace logic\n",
    "- for each chain, aggregate the daily contracts usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "from src.misc.helper_functions import get_missing_days_blockspace\n",
    "db_connector = DbConnector()\n",
    "\n",
    "chain_list = ['arbitrum', 'zksync_era', 'polygon_zkevm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last entry in blockspace_fact_contract_level detected for origin_key: imx is on 2023-07-25. Set days to 12.\n",
      "...upserting imx data . Total rows: 3881...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3881"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for chain in chain_list:\n",
    "#     days = get_missing_days_blockspace(db_connector, chain)\n",
    "\n",
    "#     ## aggregate contract data\n",
    "#     print(f\"aggregating contract data for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_contracts(chain, days)\n",
    "#     df.set_index(['address', 'date', 'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting contract data for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_contract_level', df)\n",
    "\n",
    "#     ## determine total usage\n",
    "#     print(f\"aggregating total usage for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_total(chain, days)\n",
    "#     df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting total usage usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "#     ## aggregate native transfers\n",
    "#     print(f\"aggregating native_transfers for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_native_transfers(chain, days)\n",
    "#     df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting native_transfers for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "#     ## aggregate contract deployments\n",
    "#     print(f\"aggregating smart_contract_deployments for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_contract_deplyments(chain, days)\n",
    "#     df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting smart_contract_deployments for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "#     # ALL below needs to be retriggerd when mapping changes (e.g. new addresses got labeled or new categories added etc.)\n",
    "#     ## aggregate by sub categories\n",
    "#     print(f\"aggregating sub categories for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_sub_categories(chain, days)\n",
    "#     df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting sub categories for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "#     ## determine unlabeled usage\n",
    "#     print(f\"aggregating unlabeled usage for {chain} and last {days} days...\")\n",
    "#     df = db_connector.get_blockspace_unlabeled(chain, days)\n",
    "#     df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "#     print(f\"upserting unlabeled usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "#     db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "\n",
    "days = get_missing_days_blockspace(db_connector, 'imx')\n",
    "\n",
    "df = db_connector.get_blockspace_imx(1000)\n",
    "df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "print(f\"...upserting imx data . Total rows: {df.shape[0]}...\")\n",
    "db_connector.upsert_table('blockspace_fact_sub_category_level', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_connector.get_top_contracts_by_category('main_category', 'unlabeled', 'arbitrum', 'gas', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | address                        | origin_key   | contract_name   | project_name   | sub_category_key   | sub_category_name   | main_category_key   | main_category_name   |   gas_fees_eth |   gas_fees_usd |   txcount |   daa |\n",
      "|---:|:-------------------------------|:-------------|:----------------|:---------------|:-------------------|:--------------------|:--------------------|:---------------------|---------------:|---------------:|----------:|------:|\n",
      "|  0 | <memory at 0x0000017D82E9B1C0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |       40.2063  |       74995.8  |    288129 |     1 |\n",
      "|  1 | <memory at 0x0000017D82E9B400> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        9.04008 |       16876    |     18026 |   219 |\n",
      "|  2 | <memory at 0x0000017D82E9B4C0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        7.45722 |       13909    |     54705 |  5062 |\n",
      "|  3 | <memory at 0x0000017D82E9B580> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        6.86687 |       12797    |     50520 |   180 |\n",
      "|  4 | <memory at 0x0000017D82E9B640> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        5.03925 |        9401.75 |     44871 |  5874 |\n",
      "|  5 | <memory at 0x0000017D82E9B700> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        4.61347 |        8605.65 |     23678 |  2345 |\n",
      "|  6 | <memory at 0x0000017D82E9B7C0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.34665 |        6244.32 |     32449 |  3560 |\n",
      "|  7 | <memory at 0x0000017D82E9B880> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.31957 |        6193.85 |     17138 |  1710 |\n",
      "|  8 | <memory at 0x0000017D82E9B940> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.30997 |        6174.66 |     26216 |  2569 |\n",
      "|  9 | <memory at 0x0000017D82E9BA00> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.30573 |        6167.23 |     33514 |  1841 |\n",
      "| 10 | <memory at 0x0000017D82E9BAC0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.15917 |        5889.86 |     22194 |   197 |\n",
      "| 11 | <memory at 0x0000017D82E9BB80> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        3.0198  |        5632.03 |     24107 |  2414 |\n",
      "| 12 | <memory at 0x0000017D82E9BC40> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.76977 |        5166.24 |     31972 |  1692 |\n",
      "| 13 | <memory at 0x0000017D82E9BD00> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.23922 |        4175.98 |     16098 |   987 |\n",
      "| 14 | <memory at 0x0000017D82E9BDC0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.23603 |        4172.23 |      9794 |  1185 |\n",
      "| 15 | <memory at 0x0000017D82E9BE80> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.22142 |        4144.85 |     39870 |     1 |\n",
      "| 16 | <memory at 0x0000017D82E9BF40> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.19002 |        4084.98 |     42342 |  3852 |\n",
      "| 17 | <memory at 0x0000017D83284040> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.13375 |        3977.74 |     18209 |   675 |\n",
      "| 18 | <memory at 0x0000017D83284100> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.09656 |        3906.3  |     21887 |    21 |\n",
      "| 19 | <memory at 0x0000017D832841C0> | arbitrum     |                 |                |                    |                     | unlabeled           | Unlabeled            |        2.09142 |        3899.88 |      7931 |   411 |\n"
     ]
    }
   ],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to s3 bucket and output list of files\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket('gtp-longterm')\n",
    "\n",
    "## get list of files in bucket with last modified date\n",
    "files = []\n",
    "for obj in bucket.objects.all():\n",
    "    files.append([obj.key, obj.last_modified])\n",
    "\n",
    "df = pd.DataFrame(files, columns=['key', 'last_modified'])\n",
    "\n",
    "## filter out files where key starts with 'imx'\n",
    "df = df[~df.key.str.startswith('imx')]\n",
    "\n",
    "df['chain'] = df.key.str.split('/').str[0]\n",
    "\n",
    "## create new column block_range that extracts the string between 'tx_' and '.parquet' in the key column using lambda function\n",
    "df['block_range'] = df.key.apply(lambda x: x[x.find('tx_')+3:x.find('.parquet')])\n",
    "df['block_start'] = df.block_range.str.split('-').str[0].astype(int)\n",
    "df['block_end'] = df.block_range.str.split('-').str[1].astype(int)\n",
    "\n",
    "## sort by block_start\n",
    "df.sort_values(by='block_start', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum = df[df.chain == 'arbitrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_arbitrum_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum.key.iloc[0]}\")\n",
    "df_arbitrum_flipside.sort_values(by='ETH_VALUE', inplace=True, ascending=False)\n",
    "df_arbitrum_flipside.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_arbitrum_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum[df_arbitrum.block_start >= 96528665].key.iloc[10]}\")\n",
    "df_arbitrum_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism = df[df.chain == 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_optimism_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism.key.iloc[0]}\")\n",
    "df_optimism_flipside.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_optimism_flipside[['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED',\n",
    "#        'L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]\n",
    "\n",
    "## filter df_optimism_flipside where TX_HASH = 0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559 and select columns 'TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' \n",
    "\n",
    "df_optimism_flipside[df_optimism_flipside.TX_HASH == '0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559'.lower()][['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside['GAS_PRICE'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_optimism_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism[df_optimism.block_start >= 103428989].key.iloc[0]}\")\n",
    "df_optimism_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polygon zkEVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polygon = df[df.chain == 'polygon_zkevm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_polygon_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_polygon.key.iloc[0]}\")\n",
    "df_polygon_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zkSync Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zksync = df[df.chain == 'zksync_era']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_zksync_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_zksync.key.iloc[0]}\")\n",
    "df_zksync_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## rename files in S3 bucket that contain 'block_'\n",
    "# for index, row in df[df.key.str.contains('block_')].iterrows():\n",
    "#     print(row['key'])\n",
    "#     old_key = row['key']\n",
    "#     new_key = old_key.replace('block_', '')\n",
    "#     print(new_key)\n",
    "#     s3.Object('gtp-longterm', new_key).copy_from(CopySource='gtp-longterm/'+old_key)\n",
    "#     #s3.Object('gtp-longterm', old_key).delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum[df_arbitrum.block_start >= 96528665].sort_values(by='last_modified', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_zksync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexed.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "aws_access_key_id = \"43c31ff797ec2387177cabab6d18f15a\"\n",
    "aws_secret_access_key = \"afb354f05026f2512557922974e9dd2fdb21e5c2f5cbf929b35f0645fb284cf7\"\n",
    "bucket_name = 'indexed-xyz'\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list all files in bucket\n",
    "for obj in s3.list_objects(Bucket=bucket_name)['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
