{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_l2beat import AdapterL2Beat\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['optimism','arbitrum'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterL2Beat(adapter_params, db_connector)\n",
    "# extract\n",
    "df= ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefiLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_defillama import AdapterDefiLlama\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['ethereum'] # see all options in adapter_mapping.py\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDefiLlama(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coingecko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for projects / chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'project',\n",
    "    'metric_keys' : ['price', 'volume', 'market_cap'],\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['aptos'], # see all options in adapter_mapping.py\n",
    "    'days' : 'auto', # auto, max, or a number (as string)\n",
    "    'vs_currencies' : ['usd', 'eth']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for imx tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'imx_tokens'\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    # 'query_names' : ['waa'], ## fundamentals, waa, stables_mcap\n",
    "    # 'days' : 100,\n",
    "    'query_names' : None,\n",
    "    'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside\n",
    "sometimes some Flipside queries just get stuck -- gotta retrigger them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_flipside import AdapterFlipside\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : ['zksync_era'],\n",
    "    'metric_keys' : ['stables_mcap'],\n",
    "    'days' : 'auto',\n",
    "    # 'origin_keys' : None,\n",
    "    # 'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipside(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing the new flipside api\n",
    "\n",
    "from flipside import Flipside\n",
    "flipside = Flipside(\"74c43ebc-3291-4953-8aeb-65640da7c852\", \"https://api-v2.flipsidecrypto.xyz\")\n",
    "\n",
    "sql = \"\"\"\n",
    " select \n",
    "    BLOCK_NUMBER, BLOCK_TIMESTAMP, BLOCK_HASH, TX_HASH, NONCE, POSITION, ORIGIN_FUNCTION_SIGNATURE, FROM_ADDRESS, TO_ADDRESS, ETH_VALUE, TX_FEE, GAS_PRICE, GAS_LIMIT, \n",
    "    GAS_USED, L1_GAS_PRICE, L1_GAS_USED, L1_FEE_SCALAR, L1_SUBMISSION_BATCH_INDEX, L1_SUBMISSION_TX_HASH, L1_STATE_ROOT_BATCH_INDEX, \n",
    "    L1_STATE_ROOT_TX_HASH, CUMULATIVE_GAS_USED, INPUT_DATA, STATUS\n",
    "from optimism.core.fact_transactions\n",
    "where block_number >= 104130000\n",
    "and block_number < 104135000\n",
    "order by block_number asc\n",
    "\"\"\"\n",
    "\n",
    "# Run the query against Flipside's query engine and await the results\n",
    "query_result_set = flipside.query(sql)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zettablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_zettablock import AdapterZettablock\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : ['zksync_era', 'polygon_zkevm'],\n",
    "    #'metric_keys' : ['txcount', ''],\n",
    "    'days' : 10,\n",
    "    'origin_keys' : None,\n",
    "    'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettablock(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZettaBlock raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_zettablock import AdapterZettaBlockRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    #'keys' : ['polygon_zkevm_tx', 'zksync_era_tx'],\n",
    "    'keys' : ['zksync_era_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettaBlockRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params, if_exists = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainbase raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_chainbase import AdapterChainbaseRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"CHAINBASE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 64900000, ## until 65,570,000\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterChainbaseRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMX raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement orchestration?\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_imx import AdapterRawImx\n",
    "\n",
    "adapter_params = {\n",
    "    'load_types' : ['withdrawals', 'deposits', 'trades', 'orders_filled', 'transfers', 'mints'],\n",
    "    'forced_refresh' : 'no',\n",
    "\n",
    "    #'load_types' : ['orders_filled'],\n",
    "    #'forced_refresh' : '2023-04-01T00:00:00.000000Z',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRawImx(adapter_params, db_connector)\n",
    "# extract raw (and load raw in case of IMX)\n",
    "df_raw = ad.extract_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flipside raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_flipside import AdapterFlipsideRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"FLIPSIDE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx', 'optimism_tx'],\n",
    "    'block_start' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterFlipsideRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads currently IMX txcount, daa, fees_paid\n",
    "## also loads user_base_weekly\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics', ## usd_to_eth or metrics\n",
    "    \n",
    "    #'days' : 'auto', ## days as int our 'auto\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    #'metric_keys' : None, ## metric_keys as list or None\n",
    "\n",
    "    'days' : '28', ## days as int our 'auto\n",
    "    #'origin_keys' : ['imx'], ## origin_keys as list or None\n",
    "    'metric_keys' : ['user_base_weekly'], ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.origin_key.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'usd_to_eth', ## usd_to_eth or metrics\n",
    "    'days' : 180, ## days as int\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockspace logic\n",
    "- for each chain, aggregate the daily contracts usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "\n",
    "chain_list = ['arbitrum', 'zksync_era', 'polygon_zkevm']\n",
    "chain_list = ['arbitrum']\n",
    "days = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating contract data for arbitrum and last 3000 days...\n",
      "upserting contract data for arbitrum. Total rows: 1060083...\n",
      "Batch upload necessary. Total size: 1060083\n",
      "Batch 100000\n",
      "Batch 200000\n",
      "Batch 300000\n",
      "Batch 400000\n",
      "Batch 500000\n",
      "Batch 600000\n",
      "Batch 700000\n",
      "Batch 800000\n",
      "Batch 900000\n",
      "Batch 1000000\n",
      "Batch 1100000\n",
      "aggregating native_transfers for arbitrum and last 3000 days...\n",
      "upserting native_transfers for arbitrum. Total rows: 461...\n",
      "aggregating sub categories for arbitrum and last 3000 days...\n",
      "upserting sub categories for arbitrum. Total rows: 6819...\n",
      "aggregating unlabeled usage for arbitrum and last 3000 days...\n",
      "upserting unlabeled usage for arbitrum. Total rows: 461...\n"
     ]
    }
   ],
   "source": [
    "for chain in chain_list:\n",
    "    ## aggregate contract data\n",
    "    print(f\"aggregating contract data for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_contracts(chain, days)\n",
    "    df.set_index(['address', 'date', 'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting contract data for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_contract_level', df)\n",
    "\n",
    "    ## aggregate native transfers\n",
    "    print(f\"aggregating native_transfers for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_native_transfers(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting native_transfers for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "\n",
    "    # ALL below needs to be retriggerd when mapping changes (e.g. new addresses got labeled or new categories added etc.)\n",
    "    ## aggregate by sub categories\n",
    "    print(f\"aggregating sub categories for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_sub_categories(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting sub categories for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    ## determine unlabeled usage\n",
    "    print(f\"aggregating unlabeled usage for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_unlabeled(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    print(f\"upserting unlabeled usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    db_connector.upsert_table('blockspace_fact_sub_category_level', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df order by block_number asc\n",
    "df.sort_values(by=['date'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take first 50000 rows from df and store in df1\n",
    "df1 = df.iloc[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.set_index(['address', 'date', 'origin_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"upserting contract data for {chain}. Total rows: {df1.shape[0]}...\")\n",
    "db_connector.upsert_table('blockspace_fact_contract_level', df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to s3 bucket and output list of files\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket('gtp-longterm')\n",
    "\n",
    "## get list of files in bucket with last modified date\n",
    "files = []\n",
    "for obj in bucket.objects.all():\n",
    "    files.append([obj.key, obj.last_modified])\n",
    "\n",
    "df = pd.DataFrame(files, columns=['key', 'last_modified'])\n",
    "\n",
    "## filter out files where key starts with 'imx'\n",
    "df = df[~df.key.str.startswith('imx')]\n",
    "\n",
    "df['chain'] = df.key.str.split('/').str[0]\n",
    "\n",
    "## create new column block_range that extracts the string between 'tx_' and '.parquet' in the key column using lambda function\n",
    "df['block_range'] = df.key.apply(lambda x: x[x.find('tx_')+3:x.find('.parquet')])\n",
    "df['block_start'] = df.block_range.str.split('-').str[0].astype(int)\n",
    "df['block_end'] = df.block_range.str.split('-').str[1].astype(int)\n",
    "\n",
    "## sort by block_start\n",
    "df.sort_values(by='block_start', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum = df[df.chain == 'arbitrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_arbitrum_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum.key.iloc[0]}\")\n",
    "df_arbitrum_flipside.sort_values(by='ETH_VALUE', inplace=True, ascending=False)\n",
    "df_arbitrum_flipside.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_arbitrum_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum[df_arbitrum.block_start >= 96528665].key.iloc[10]}\")\n",
    "df_arbitrum_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism = df[df.chain == 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_optimism_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism.key.iloc[0]}\")\n",
    "df_optimism_flipside.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_optimism_flipside[['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED',\n",
    "#        'L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]\n",
    "\n",
    "## filter df_optimism_flipside where TX_HASH = 0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559 and select columns 'TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' \n",
    "\n",
    "df_optimism_flipside[df_optimism_flipside.TX_HASH == '0xA5E0DACD8D29020C04139F8506C01CBA8B6E561CE567DF8DA35857722232F559'.lower()][['TX_HASH', 'ETH_VALUE', 'TX_FEE', 'GAS_PRICE', 'GAS_LIMIT', 'GAS_USED','L1_GAS_PRICE', 'L1_GAS_USED', 'L1_FEE_SCALAR',  'CUMULATIVE_GAS_USED' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside['GAS_PRICE'] / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimism_flipside.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_optimism_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_optimism[df_optimism.block_start >= 103428989].key.iloc[0]}\")\n",
    "df_optimism_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polygon zkEVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_polygon = df[df.chain == 'polygon_zkevm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_polygon_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_polygon.key.iloc[0]}\")\n",
    "df_polygon_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zkSync Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zksync = df[df.chain == 'zksync_era']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum into df\n",
    "df_zksync_zb = pd.read_parquet(f\"s3://gtp-longterm/{df_zksync.key.iloc[0]}\")\n",
    "df_zksync_zb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## rename files in S3 bucket that contain 'block_'\n",
    "# for index, row in df[df.key.str.contains('block_')].iterrows():\n",
    "#     print(row['key'])\n",
    "#     old_key = row['key']\n",
    "#     new_key = old_key.replace('block_', '')\n",
    "#     print(new_key)\n",
    "#     s3.Object('gtp-longterm', new_key).copy_from(CopySource='gtp-longterm/'+old_key)\n",
    "#     #s3.Object('gtp-longterm', old_key).delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum[df_arbitrum.block_start >= 96528665].sort_values(by='last_modified', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load polygon zkevm data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "from sqlalchemy import create_engine, exc\n",
    "import sys\n",
    "from pangres import upsert\n",
    "\n",
    "# Load environment variables\n",
    "def load_environment():\n",
    "    load_dotenv()\n",
    "\n",
    "    # Postgres details from .env file\n",
    "    db_user = os.getenv(\"DB_USERNAME\")\n",
    "    db_password = os.getenv(\"DB_PASSWORD\")\n",
    "    db_host = os.getenv(\"DB_HOST\")\n",
    "    db_database = os.getenv(\"DB_DATABASE\")\n",
    "\n",
    "    return db_user, db_password, db_host, db_database\n",
    "\n",
    "def connect_to_s3():\n",
    "    try:\n",
    "        aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "        aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        bucket_name = 'gtp-longterm'\n",
    "\n",
    "        if not aws_access_key_id or not aws_secret_access_key or not bucket_name:\n",
    "            raise ValueError(\"AWS access key ID, secret access key, or bucket name not found in environment variables.\")\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        )\n",
    "        return s3, bucket_name\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while connecting to S3:\", str(e))\n",
    "        return None, None\n",
    "\n",
    "def get_files_from_bucket(s3, bucket_name, folder_key):\n",
    "    return s3.list_objects(Bucket=bucket_name, Prefix=folder_key)\n",
    "\n",
    "def load_parquet_from_s3(s3, bucket_name, file_key):\n",
    "    print(f\"Loading file {file_key} from bucket {bucket_name}...\")\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        df = pd.read_parquet(BytesIO(obj['Body'].read()))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file {file_key} from bucket {bucket_name}.\")\n",
    "        print(e)\n",
    "\n",
    "def create_db_engine(db_user, db_password, db_host, db_database):\n",
    "    print(\"Creating database engine...\")\n",
    "    try:\n",
    "        # create connection to Postgres\n",
    "        engine = create_engine(f'postgresql://{db_user}:{db_password}@{db_host}/{db_database}')\n",
    "        engine.connect()  # test connection\n",
    "        return engine\n",
    "    except exc.SQLAlchemyError as e:\n",
    "        print(\"Error connecting to database. Check your database configurations.\")\n",
    "        print(e)\n",
    "        raise ValueError(\"Error inserting data into table.\")\n",
    "\n",
    "def insert_into_db(df, engine, table_name):\n",
    "    # Set the DataFrame's index to 'tx_hash' (primary key)\n",
    "    df.set_index('tx_hash', inplace=True)\n",
    "    df.index.name = 'tx_hash'\n",
    "    \n",
    "    # Insert data into database\n",
    "    print(f\"Inserting data into table {table_name}...\")\n",
    "    try:\n",
    "        upsert(engine, df, table_name, if_row_exists='ignore')\n",
    "        print(\"Data inserted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting data into table {table_name}.\")\n",
    "        print(e)\n",
    "        print(df)\n",
    "        raise ValueError(\"Error inserting data into table.\")\n",
    "\n",
    "def process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, start_index):\n",
    "    # if start_index is less than 0 or greater than the number of files, set it to 0\n",
    "    start_index = max(0, min(start_index, len(files_in_bucket['Contents']) - 1))\n",
    "    #for file in files_in_bucket['Contents'][:10]:  # Only process first 10 files\n",
    "    for file in files_in_bucket['Contents'][start_index:]:\n",
    "        # Print file index in the list\n",
    "        print(f\"--> Processing file {files_in_bucket['Contents'].index(file) + 1} of {len(files_in_bucket['Contents'])}...\")\n",
    "        if file['Key'].endswith('.parquet'):  # to ensure only .parquet files are processed\n",
    "            # Parse the min and max block numbers from the filename\n",
    "            filename = os.path.basename(file['Key'])\n",
    "            min_block, max_block = filename.rsplit('_', 1)[1].split('-')[0], filename.rsplit('_', 1)[1].split('-')[1].split('.')[0]\n",
    "            print(f\"--> Processing file {filename} with block range {min_block} - {max_block}\")\n",
    "\n",
    "            df = load_parquet_from_s3(s3, bucket_name, file['Key'])\n",
    "            if df is not None:\n",
    "                print(f\"Preparing DataFrame for file {filename}...\")\n",
    "                if table_name == 'polygon_zkevm_tx':\n",
    "                    df = prepare_dataframe_polygon(df)  # call to the function that prepares the dataframe\n",
    "                elif table_name == 'zksync_era_tx':\n",
    "                    df = prepare_dataframe_zksync(df)  # call to the function that prepares the dataframe\n",
    "                insert_into_db(df, engine, table_name)\n",
    "\n",
    "def prepare_dataframe_polygon(df):\n",
    "    print(\"Preparing DataFrame for polygon_zkevm_tx...\")\n",
    "    # Columns to be used from the dataframe\n",
    "    cols = ['block_number', 'block_time', 'hash', 'from_address', 'to_address', 'status', 'value', 'gas_limit', 'gas_used', 'gas_price', 'type', 'receipt_contract_address']\n",
    "\n",
    "    # Filter the dataframe to only include the above columns plus 'input' for calculations\n",
    "    df = df.loc[:, cols + ['input']]\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'hash': \"tx_hash\", \"block_time\":\"block_timestamp\"}, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Add tx_fee column\n",
    "    df['tx_fee'] = df['gas_used'] * df['gas_price']  / 1e18\n",
    "\n",
    "    # gas_price column: convert to eth\n",
    "    df['gas_price'] = df['gas_price'].astype(float) / 1e18\n",
    "\n",
    "    # Add native_transfer column True when (input = 0x OR input is empty) and value > 0 then true else false\n",
    "    df['value'] = df['value'].astype(float) / 1e18\n",
    "\n",
    "    # Add empty_input column True when input is empty or 0x then true else false\n",
    "    df['empty_input'] = df['input'].apply(lambda x: True if (x == '0x' or x == '') else False)\n",
    "\n",
    "    # status column: 1 if status is success, 0 if failed else -1\n",
    "    df['status'] = df['status'].apply(lambda x: 1 if x == 1 else 0 if x == 0 else -1)\n",
    "\n",
    "    # Convert block_time to datetime\n",
    "    df['block_timestamp'] = pd.to_datetime(df['block_timestamp'])\n",
    "\n",
    "    # Handle bytea data type\n",
    "    for col in ['tx_hash', 'to_address', 'from_address', 'receipt_contract_address']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.replace('0x', '\\\\x', regex=False)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in dataframe.\")\n",
    "    \n",
    "    # Drop the 'input' column\n",
    "    df = df.drop(columns=['input'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_dataframe_zksync(df):\n",
    "    print(\"Preparing DataFrame for zksync_era_tx...\")\n",
    "\n",
    "    # Columns to be used from the dataframe\n",
    "    cols = ['block_number', 'block_time', 'hash', 'from_address', 'to_address', 'status', 'value', 'gas_limit', 'gas_used', 'gas_price', 'type', 'receipt_contract_address']\n",
    "\n",
    "    # Filter the dataframe to only include the above columns plus 'input' for calculations\n",
    "    df = df.loc[:, cols + ['input']]\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(columns={'hash': \"tx_hash\", \"block_time\":\"block_timestamp\"}, inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Add tx_fee column\n",
    "    df['tx_fee'] = df['gas_used'] * df['gas_price']  / 1e18\n",
    "\n",
    "    # gas_price column: convert to eth\n",
    "    df['gas_price'] = df['gas_price'].astype(float) / 1e18\n",
    "\n",
    "    # value column divide by 1e18 to convert to eth\n",
    "    df['value'] = df['value'].astype(float) / 1e18\n",
    "\n",
    "    # Add empty_input column True when input is empty or 0x then true else false\n",
    "    df['empty_input'] = df['input'].apply(lambda x: True if (x == '0x' or x == '') else False)\n",
    "\n",
    "    # status column: 1 if status is success, 0 if failed else -1\n",
    "    df['status'] = df['status'].apply(lambda x: 1 if x == 1 else 0 if x == 0 else -1)\n",
    "\n",
    "    # Convert block_time to datetime\n",
    "    df['block_timestamp'] = pd.to_datetime(df['block_timestamp'])\n",
    "\n",
    "    # Handle bytea data type\n",
    "    for col in ['tx_hash', 'to_address', 'from_address', 'receipt_contract_address']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('0x', '\\\\x', regex=False)\n",
    "        else:\n",
    "            print(f\"Column {col} not found in dataframe.\")\n",
    "    \n",
    "    # Drop the 'input' column\n",
    "    df = df.drop(columns=['input'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main_polygon():\n",
    "    # List of folders in the S3 bucket\n",
    "    chain_folders = ['arbitrum', 'imx', 'optimism', 'polygon_zkevm', 'zksync_era']\n",
    "    \n",
    "    # Load environment variables\n",
    "    db_user, db_password, db_host, db_database = load_environment()\n",
    "\n",
    "    # Create DB engine\n",
    "    engine = create_db_engine(db_user, db_password, db_host, db_database)\n",
    "\n",
    "    # Connect to S3 bucket\n",
    "    s3, bucket_name = connect_to_s3()\n",
    "\n",
    "    # Select the desired chain folder\n",
    "    folder_key = chain_folders[3]+'/'\n",
    "\n",
    "    # Get list of all files in the specific S3 bucket and folder\n",
    "    files_in_bucket = get_files_from_bucket(s3, bucket_name, folder_key)\n",
    "\n",
    "    # Sort the files by min_block\n",
    "    files_in_bucket['Contents'].sort(key=lambda x: int(os.path.basename(x['Key']).rsplit('_', 1)[1].split('-')[0]))\n",
    "\n",
    "    # Table name in the database\n",
    "    table_name = 'polygon_zkevm_tx'\n",
    "\n",
    "    # Process the files in the bucket\n",
    "    process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, 0)\n",
    "\n",
    "def main_zksync():\n",
    "    # List of folders in the S3 bucket\n",
    "    chain_folders = ['arbitrum', 'imx', 'optimism', 'polygon_zkevm', 'zksync_era']\n",
    "    \n",
    "    # Load environment variables\n",
    "    db_user, db_password, db_host, db_database = load_environment()\n",
    "\n",
    "    # Create DB engine\n",
    "    engine = create_db_engine(db_user, db_password, db_host, db_database)\n",
    "\n",
    "    # Connect to S3 bucket\n",
    "    s3, bucket_name = connect_to_s3()\n",
    "\n",
    "    # Select the desired chain folder\n",
    "    folder_key = chain_folders[4]+'/'\n",
    "\n",
    "    # Get list of all files in the specific S3 bucket and folder\n",
    "    files_in_bucket = get_files_from_bucket(s3, bucket_name, folder_key)\n",
    "\n",
    "    # Sort the files by min_block\n",
    "    files_in_bucket['Contents'].sort(key=lambda x: int(os.path.basename(x['Key']).rsplit('_', 1)[1].split('-')[0]))\n",
    "\n",
    "    # Table name in the database\n",
    "    table_name = 'zksync_era_tx'\n",
    "\n",
    "    # Process the files in the bucket\n",
    "    process_files_in_bucket(files_in_bucket, s3, bucket_name, engine, table_name, 981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_zksync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
