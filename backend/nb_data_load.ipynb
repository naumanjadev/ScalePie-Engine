{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DELETE\n",
    "FROM public.fact_kpis\n",
    "where origin_key = 'linea' and \"date\" < '2023-7-18' and metric_key in ('txcosts_median_usd', 'txcosts_median_eth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "\n",
    "db_connector = DbConnector()\n",
    "\n",
    "df = db_connector.get_blockspace_contracts('optimism', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "import pandas as pd \n",
    "\n",
    "db_connector = DbConnector()\n",
    "\n",
    "exec_string = '''\n",
    "    select from_address, count(*)\n",
    "    from optimism_tx\n",
    "    group by 1\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(exec_string, db_connector.engine.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from_address column from bytea to text\n",
    "df['from_address'] = df['from_address'].apply(lambda x: x.hex() if x is not None else None)\n",
    "## add 0x to from_address\n",
    "df['from_address'] = '0x' + df['from_address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('local/optimism_block_1-114871709.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.adapters.mapping import adapter_mapping\n",
    "\n",
    "print([chain.origin_key for chain in adapter_mapping if chain.in_api == True and 'blockspace' not in chain.exclude_metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## web3 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web3 import Web3\n",
    "import os\n",
    "\n",
    "tx = '0xdba1ec7832bc857a4f3624557572f24169dda03501fd2b6d6a529647c9a64c51'\n",
    "url = \"https://rpc.zora.energy/\"\n",
    "#url = f\"https://rpc.ankr.com/base/{os.getenv('ANKR_API')}\"\n",
    "w3 = Web3(Web3.HTTPProvider(url))\n",
    "\n",
    "block = w3.eth.get_block(3913960, full_transactions=True)\n",
    "receipt = w3.eth.get_transaction_receipt(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.misc.helper_functions import api_post_call\n",
    "import json\n",
    " \n",
    "payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getTransactionReceipt\",\n",
    "        \"params\": [tx],\n",
    "        \"id\": 1\n",
    "    }\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = api_post_call(url, payload=json.dumps(payload), header=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_number_hex = hex(3913960)\n",
    "payload = {\n",
    "    \"jsonrpc\": \"2.0\",\n",
    "    \"method\": \"eth_getBlockByNumber\",\n",
    "    \"params\": [str(block_number_hex), True],\n",
    "    \"id\": 1\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = api_post_call(url, payload=json.dumps(payload), header=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.api.json_creation import JSONCreation\n",
    "from src.db_connector import DbConnector\n",
    "\n",
    "db_connector = DbConnector()\n",
    "\n",
    "json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"v1\")\n",
    "## for testing\n",
    "#json_creator = JSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, \"test\")\n",
    "\n",
    "#json_creator.create_all_jsons()\n",
    "\n",
    "df = json_creator.get_all_data()\n",
    "# json_creator.create_landingpage_json(df)\n",
    "#json_creator.create_chain_details_jsons(df)\n",
    "json_creator.create_metric_details_jsons(df)\n",
    "# json_creator.create_fundamentals_json(df)\n",
    "#json_creator.create_master_json()\n",
    "# #json_creator.create_contracts_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "from src.api.blockspace_json_creation import BlockspaceJSONCreation\n",
    "db_connector = DbConnector()\n",
    "api_version = \"v1\"\n",
    "\n",
    "blockspace_json_creator = BlockspaceJSONCreation(os.getenv(\"S3_CF_BUCKET\"), os.getenv(\"CF_DISTRIBUTION_ID\"), db_connector, api_version)\n",
    "blockspace_json_creator.create_blockspace_overview_json()\n",
    "blockspace_json_creator.create_blockspace_comparison_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Check Data from Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_cross_check import AdapterCrossCheck\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'origin_keys' : None,\n",
    "    #'origin_keys' : ['zksync_era'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCrossCheck(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)\n",
    "\n",
    "##ad.cross_check()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2Beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_l2beat import AdapterL2Beat\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : None,\n",
    "    'origin_keys' : ['loopring'] # see all options in adapter_mapping.py \n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterL2Beat(adapter_params, db_connector)\n",
    "# extract\n",
    "df= ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefiLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_defillama import AdapterDefiLlama\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : None,\n",
    "    'origin_keys' : ['loopring'] # see all options in adapter_mapping.py\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDefiLlama(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coingecko"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run for projects / chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'project',\n",
    "    'metric_keys' : ['price', 'volume', 'market_cap'],\n",
    "   # 'origin_keys' : None,\n",
    "    'origin_keys' : ['loopring'], # see all options in adapter_mapping.py\n",
    "    'days' : 'auto', # auto, max, or a number (as string)\n",
    "    'vs_currencies' : ['usd', 'eth']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for imx tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_coingecko import AdapterCoingecko\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "load_params = {\n",
    "    'load_type' : 'imx_tokens'\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterCoingecko(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics',\n",
    "    'query_names' : ['maa'], ## fundamentals, waa, stables_mcap, rent_paid\n",
    "    'days' : 5000,\n",
    "    #'query_names' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "#ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_dune import AdapterDune\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"DUNE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'inscriptions',\n",
    "    'days' : 1300,\n",
    "    'query_names' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterDune(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# upload\n",
    "ad.load(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zettablock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_zettablock import AdapterZettablock\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "load_params = {\n",
    "    #'origin_keys' : ['zksync_era', 'polygon_zkevm'],\n",
    "    'metric_keys' : ['fees_paid_eth', 'txcosts_median_eth'],\n",
    "    'days' : 5,\n",
    "    'origin_keys' : None,\n",
    "    #'metric_keys' : None,\n",
    "    # 'days' : 'auto',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettablock(adapter_params, db_connector)\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPC raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_rpc import AdapterRPCRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'ankr',\n",
    "    'api_key' : os.getenv(\"ANKR_API\"),\n",
    "    'chain' : 'base'\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    #'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "    'batch_size' : 10,\n",
    "    'threads' : 5\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRPCRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_rpc import AdapterRPCRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'ankr',\n",
    "    'api_key' : os.getenv(\"ANKR_API\"),\n",
    "    'chain' : 'optimism'\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    #'block_start' : 9137631, ## 'auto' or a block number as int\n",
    "    'batch_size' : 10,\n",
    "    'threads' : 5\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRPCRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZettaBlock raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_zettablock import AdapterZettaBlockRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"ZETTABLOCK_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    #'keys' : ['polygon_zkevm_tx', 'zksync_era_tx'],\n",
    "    'keys' : ['polygon_zkevm_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 8456353, ## 'auto' or a block number as int\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterZettaBlockRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params, if_exists = 'ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainbase raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_chainbase import AdapterChainbaseRaw\n",
    "\n",
    "adapter_params = {\n",
    "    'api_key' : os.getenv(\"CHAINBASE_API\")\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'keys' : ['arbitrum_tx'],\n",
    "    #'block_start' : 'auto', ## 'auto' or a block number as int\n",
    "    'block_start' : 64900000, ## until 65,570,000\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterChainbaseRaw(adapter_params, db_connector)\n",
    "# extract\n",
    "ad.extract_raw(load_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMX raw data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement orchestration?\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_raw_imx import AdapterRawImx\n",
    "\n",
    "adapter_params = {\n",
    "    'load_types' : ['withdrawals', 'deposits', 'trades', 'orders_filled', 'transfers', 'mints'],\n",
    "    'forced_refresh' : 'no',\n",
    "\n",
    "    #'load_types' : ['orders_filled'],\n",
    "    #'forced_refresh' : '2023-04-01T00:00:00.000000Z',\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterRawImx(adapter_params, db_connector)\n",
    "# extract raw (and load raw in case of IMX)\n",
    "df_raw = ad.extract_raw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads currently IMX txcount, daa, fees_paid\n",
    "## also loads user_base_weekly\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'metrics', ## usd_to_eth or metrics or blockspace\n",
    "    \n",
    "    #'days' : 'auto', ## days as int our 'auto\n",
    "    'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "\n",
    "    'days' : 1500, ## days as int our 'auto\n",
    "    #'origin_keys' : ['mantle'], ## origin_keys as list or None\n",
    "    'metric_keys' : ['maa'], ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'profit', ## usd_to_eth or metrics or blockspace or profit\n",
    "    'days' : 30, ## days as int\n",
    "    #'origin_keys' : ['linea'], ## origin_keys as list or None\n",
    "    #'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'eth_to_usd', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : 35, ## days as int\n",
    "    'origin_keys' : ['mantle'], ## origin_keys as list or None\n",
    "    #'origin_keys' : None, ## origin_keys as list or None\n",
    "    'metric_keys' : None, ## metric_keys as list or None\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "df = ad.extract(load_params)\n",
    "# # load\n",
    "#ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.load(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()\n",
    "from src.adapters.adapter_sql import AdapterSQL\n",
    "\n",
    "adapter_params = {\n",
    "}\n",
    "\n",
    "load_params = {\n",
    "    'load_type' : 'blockspace', ## usd_to_eth or metrics or blockspace\n",
    "    'days' : '75', ## days as or auto\n",
    "    'origin_keys' : None ## origin_keys as list or None\n",
    "    #'origin_keys' : ['mantle']\n",
    "}\n",
    "\n",
    "# initialize adapter\n",
    "ad = AdapterSQL(adapter_params, db_connector)\n",
    "\n",
    "# extract\n",
    "ad.extract(load_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blockspace logic\n",
    "- for each chain, aggregate the daily contracts usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "from src.misc.helper_functions import get_missing_days_blockspace\n",
    "db_connector = DbConnector()\n",
    "\n",
    "chain_list = ['arbitrum']\n",
    "#chain_list = ['arbitrum', 'zksync_era', 'polygon_zkevm', 'optimism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chain in chain_list:\n",
    "    #days = get_missing_days_blockspace(db_connector, chain)\n",
    "    days = 75\n",
    "\n",
    "    # ## aggregate contract data\n",
    "    # print(f\"aggregating contract data for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_contracts(chain, days)\n",
    "    # df.set_index(['address', 'date', 'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting contract data for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_contract_level', df)\n",
    "\n",
    "    # ## determine total usage\n",
    "    # print(f\"aggregating total usage for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_total(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting total usage usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # ## aggregate native transfers\n",
    "    # print(f\"aggregating native_transfers for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_native_transfers(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting native_transfers for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    ## aggregate contract deployments\n",
    "    print(f\"aggregating smart_contract_deployments for {chain} and last {days} days...\")\n",
    "    df = db_connector.get_blockspace_contract_deplyments(chain, days)\n",
    "    df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting smart_contract_deployments for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # # ALL below needs to be retriggerd when mapping changes (e.g. new addresses got labeled or new categories added etc.)\n",
    "    # ## aggregate by sub categories\n",
    "    # print(f\"aggregating sub categories for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_sub_categories(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting sub categories for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "    # ## determine unlabeled usage\n",
    "    # print(f\"aggregating unlabeled usage for {chain} and last {days} days...\")\n",
    "    # df = db_connector.get_blockspace_unlabeled(chain, days)\n",
    "    # df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "    # print(f\"upserting unlabeled usage for {chain}. Total rows: {df.shape[0]}...\")\n",
    "    # db_connector.upsert_table('blockspace_fact_sub_category_level', df)\n",
    "\n",
    "\n",
    "# days = get_missing_days_blockspace(db_connector, 'imx')\n",
    "\n",
    "# df = db_connector.get_blockspace_imx(days)\n",
    "# df.set_index(['date', 'sub_category_key' ,'origin_key'], inplace=True)\n",
    "\n",
    "# print(f\"...upserting imx data . Total rows: {df.shape[0]}...\")\n",
    "# db_connector.upsert_table('blockspace_fact_sub_category_level', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db_connector import DbConnector\n",
    "db_connector = DbConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_connector.get_top_contracts_by_category('main_category', 'unlabeled', 'arbitrum', 'gas', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to s3 bucket and output list of files\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "bucket = s3.Bucket('gtp-longterm')\n",
    "\n",
    "## get list of files in bucket with last modified date\n",
    "files = []\n",
    "for obj in bucket.objects.all():\n",
    "    files.append([obj.key, obj.last_modified])\n",
    "\n",
    "df = pd.DataFrame(files, columns=['key', 'last_modified'])\n",
    "\n",
    "## filter out files where key starts with 'imx'\n",
    "df = df[~df.key.str.startswith('imx')]\n",
    "\n",
    "df['chain'] = df.key.str.split('/').str[0]\n",
    "\n",
    "## create new column block_range that extracts the string between 'tx_' and '.parquet' in the key column using lambda function\n",
    "df['block_range'] = df.key.apply(lambda x: x[x.find('tx_')+3:x.find('.parquet')])\n",
    "df['block_start'] = df.block_range.str.split('-').str[0].astype(int)\n",
    "df['block_end'] = df.block_range.str.split('-').str[1].astype(int)\n",
    "\n",
    "## sort by block_start\n",
    "df.sort_values(by='block_start', inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arbitrum = df[df.chain == 'arbitrum']\n",
    "\n",
    "## load first file in df_arbitrum into df\n",
    "df_arbitrum_flipside = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum.key.iloc[0]}\")\n",
    "df_arbitrum_flipside.sort_values(by='ETH_VALUE', inplace=True, ascending=False)\n",
    "df_arbitrum_flipside.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load first file in df_arbitrum with block_start >= 96528665 into df\n",
    "df_arbitrum_chainbase = pd.read_parquet(f\"s3://gtp-longterm/{df_arbitrum[df_arbitrum.block_start >= 96528665].key.iloc[10]}\")\n",
    "df_arbitrum_chainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexed.xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "aws_access_key_id = \"43c31ff797ec2387177cabab6d18f15a\"\n",
    "aws_secret_access_key = \"afb354f05026f2512557922974e9dd2fdb21e5c2f5cbf929b35f0645fb284cf7\"\n",
    "bucket_name = 'indexed-xyz'\n",
    "\n",
    "s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Set the profile name\n",
    "profile_name = 'indexedxyz'\n",
    "\n",
    "# Create an S3 client using the profile\n",
    "session = boto3.Session(profile_name=profile_name)\n",
    "s3 = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test s3 connection\n",
    "response = s3.list_objects(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## list all files in bucket\n",
    "for obj in s3.list_objects(Bucket=bucket_name)['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.adapters.adapter_raw_gtp import NodeAdapter\n",
    "from src.db_connector import DbConnector\n",
    "from datetime import datetime,timedelta\n",
    "from airflow.decorators import dag, task \n",
    "import os\n",
    "\n",
    "\n",
    "adapter_params = {\n",
    "    'rpc': 'local_node',\n",
    "    'chain': 'zora',\n",
    "    'node_url': os.getenv(\"ZORA_RPC\"),\n",
    "}\n",
    "\n",
    "# Initialize DbConnector\n",
    "db_connector = DbConnector()\n",
    "\n",
    "# Initialize NodeAdapter\n",
    "adapter = NodeAdapter(adapter_params, db_connector)\n",
    "\n",
    "# Test database connectivity\n",
    "if not adapter.check_db_connection():\n",
    "    print(\"Failed to connect to database.\")\n",
    "else:\n",
    "    print(\"Successfully connected to database.\")\n",
    "\n",
    "# Test S3 connectivity\n",
    "if not adapter.check_s3_connection():\n",
    "    print(\"Failed to connect to S3.\")\n",
    "else:\n",
    "    print(\"Successfully connected to S3.\")\n",
    "    \n",
    "# Extract\n",
    "load_params = {\n",
    "    'block_start': '4982120',\n",
    "    'batch_size': 1,\n",
    "    'threads': 1,\n",
    "}\n",
    "adapter.extract_raw(load_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We <3 the Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load last 100 tx from tx_optimism\n",
    "\n",
    "from src.db_connector import DbConnector\n",
    "import pandas as pd\n",
    "db_connector = DbConnector()\n",
    "\n",
    "exec_string = \"\"\"  select to_address, from_address from optimism_tx where to_address is not null order by block_number desc limit 10000 \"\"\"\n",
    "\n",
    "df = pd.read_sql(exec_string, db_connector.engine.connect())\n",
    "\n",
    "## to_address and from_address are bytearrays - convert to string (if not none)\n",
    "df['to_address'] = df['to_address'].apply(lambda x: x.hex() if x is not None else None)\n",
    "df['from_address'] = df['from_address'].apply(lambda x: x.hex() if x is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming df is your DataFrame with 'from_address' and 'to_address' columns\n",
    "G = nx.from_pandas_edgelist(df, 'from_address', 'to_address')\n",
    "\n",
    "# Get positions for the nodes in G\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Create Edges\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.2, color='#ff0420'), hoverinfo='none', mode='lines')\n",
    "\n",
    "# Create Nodes\n",
    "node_x = [pos[node][0] for node in G.nodes()]\n",
    "node_y = [pos[node][1] for node in G.nodes()]\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        # showscale=True,\n",
    "        # # colorscale options\n",
    "        # #'Greys' | 'YlGnBu' | 'Greens' | 'YlOrRd' | 'Bluered' | 'RdBu' |\n",
    "        # #'Reds' | 'Blues' | 'Picnic' | 'Rainbow' | 'Portland' | 'Jet' |\n",
    "        # #'Hot' | 'Blackbody' | 'Earth' | 'Electric' | 'Viridis' |\n",
    "        # colorscale='Reds',\n",
    "        # reversescale=False,\n",
    "        color=\"#ff0420\",\n",
    "        size=7,\n",
    "        line_width=1))\n",
    "\n",
    "node_adjacencies = []\n",
    "node_text = []\n",
    "for node, adjacencies in enumerate(G.adjacency()):\n",
    "    node_adjacencies.append(len(adjacencies[1]))\n",
    "    node_text.append('# of connections: '+str(len(adjacencies[1])))\n",
    "\n",
    "node_trace.text = node_text\n",
    "\n",
    "# Create Network Graph\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "             layout=go.Layout(\n",
    "                title='Optimism WE <3 THE ART | Just the last 10k Transactions',\n",
    "                titlefont_size=14,\n",
    "                showlegend=False,\n",
    "                hovermode='closest',\n",
    "                margin=dict(b=20,l=5,r=5,t=40),\n",
    "                annotations=[ dict(\n",
    "                    text=\"@web3_data</a>\",\n",
    "                    showarrow=False,\n",
    "                    xref=\"paper\", yref=\"paper\",\n",
    "                    x=0.01, y=-0.002 ) ],\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "                )\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=800,\n",
    "    plot_bgcolor='white',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
